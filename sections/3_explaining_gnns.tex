\section{Explaining deep graph models}
\label{s_ExplainingGNNs}

%No longer purely related work. First outline explainable ai in general, then transition to gnns and the cummulate into the next chapter on explanability methods on gnns. Last part of the chapeter is the related work subchapter

% Introduction into related work, laying out the general story of the chapter: From explanations on static graphs and the different approaches to more detailed description of highly related work on explanations on dynamic graphs

Alongside the rise in adaptation of artificial intelligence and deep learning methods over recent years, the idea of explainable artificial intelligence has been gaining increasing attention \cite{adadi_peeking_2018}. The field of explainable artificial intelligence focuses on providing transparency over the mechanisms that underlie artificial models \cite{barredo_arrieta_explainable_2020}. Many machine learning models behave like black-boxes, meaning that the internal mechanisms that lead to their predictions remain opaque to their users \cite{prado-romero_survey_2023}. Explainable artificial intelligence differentiates \textit{interpretability} from \textit{explainability}.
Interpretability refers to the degree to which the internal workings and decision-making processes of an artificial intelligence model can be understood by humans by design \cite{barredo_arrieta_explainable_2020}.
In comparison, explainability refers to the ability of an artificial intelligence system to provide clear, coherent, and accessible explanations for its decisions or predictions, offering insights into the rationale behind specific outcomes in a human-understandable manner \cite{barredo_arrieta_explainable_2020}. While interpretable \gls{gnn} models utilize mechanisms that are easy to understand on their own \cite{prado-romero_survey_2023}, black-box \gls{gnn} models have shown better performance on high-dimensional input data than interpretable models \cite{prado-romero_survey_2023}. Thus, proper explanation methods are required to make black-box models explainable.

%Transition to explanability for gnns and outline the structure of this chapter.

Even though most works on static \glspl{gnn} have only emerged in the past 10 years \cite{wu_comprehensive_2021}, the black-box nature of the majority of these \glspl{gnn} has given rise to a plethora of explanation methods. The explanation approaches that have been proposed vary in many ways, like the type of explanation provided, the mechanism by which they explain produce explanations, or if and how explainers are trained \cite{kakkad_survey_2023}. Figure \ref{f_taxonomy_explainers} provides a classification of these varied approaches. On a conceptual level, model-level and instance-level approaches are differentiated \cite{yuan_explainability_2020}. Model-level methods explain \glspl{gnn} as a whole, irrespective of any particular input example \cite{yuan_explainability_2020}. Such explanations aim to uncover a high-level understanding of the inner workings of the explained \gls{gnn}. In comparison, instance-level methods explain the output of a model for a specific input instance \cite{yuan_explainability_2020}. These explanations are tailored to individual instances, offering a fine-grained understanding of the reasoning behind each prediction.

\begin{figure}[ht]
    \centering
    \include{figures/explainer_taxonomy}
    \caption{Taxonomy of \gls{gnn} explanation methods. Explanation approaches are categorized into instance-level and model-level methods. The category of instance-level methods is further subdivided into factual and counterfactual approaches. Taxonomy adapted from \cite{yuan_explainability_2020}, \cite{prado-romero_survey_2023}, and \cite{kakkad_survey_2023}.}
    \label{f_taxonomy_explainers}

\end{figure}

\textit{Model-level} explainers like XGNN \cite{yuan_xgnn_2020} map the behavior of a \gls{gnn} to patterns in the input graphs \cite{yuan_explainability_2020}. These explainers use a so-called generation approach, which explains \glspl{gnn} by training a model to generate graphs that maximize a particular prediction of the \gls{gnn} \cite{yuan_explainability_2020, yuan_xgnn_2020}. The thereby generated graph patterns are then used as an explanation \cite{yuan_xgnn_2020}. While these approaches can provide high-level explanations for the general model behavior, they fail to explain the specifics of why certain predictions are made for given inputs and are thus not discussed any further.

\textit{Instance-level} approaches are subdivided into those that provide a factual and those that provide a counterfactual explanation. On a fundamental level, factual reasoning inquires, "With A already having occurred, will B occur?" \cite{quelhas_relation_2018, tan_learning_2022}. Hence, a factual explanation states that a prediction for B was made because A occurred. In contrast, counterfactual reasoning poses the question, "If A did not occur, would B still happen?" \cite{quelhas_relation_2018, tan_learning_2022}. A counterfactual explanation is thus interested in the information A, whose non-existence would entail a different prediction for the occurrence of B. The following sections take a closer look at explanation methods using either type of reasoning.

\subsection{Factual Explanation Approaches}
\label{s_ExplainingGNNs_Factual}

Factual explanations for \glspl{gnn} answer the question: "Given the input A, will the \gls{gnn} predict B?". These explanations seek the input information with the maximum influence on the \gls{gnn}'s prediction \cite{kakkad_survey_2023}. They can take the form of a subgraph $G' \subseteq G$ of the original input $G$ \cite{kakkad_survey_2023} that is \textit{sufficient} to produce the original prediction with the targeted \gls{gnn} $f(\cdot)$ \cite{tan_learning_2022}. For a sufficient factual explanation, the following equation holds:

\begin{equation}
    f(G) = f(G')
\end{equation}

\vspace{0.5cm}

As depicted in Figure \ref{f_taxonomy_explainers}, Factual explainers for \glspl{gnn} are categorized based on the primary mechanisms they employ to explain predictions, each offering different approaches and forms of explanations:

\begin{itemize}
    \item \textbf{Gradient/Feature} based methods identify important features by analyzing how changes in input features affect the network's predictions \cite{yuan_explainability_2020}. These approaches either employ back-propagation to examine the gradients of the explained prediction concerning the input features or map hidden features through interpolation into the input space \cite{yuan_explainability_2020}. Examples of these methods are Class Activation Mapping (CAM)\cite{pope_explainability_2019} and Sensitivity Analysis (SA) \cite{baldassarre_explainability_2019}.
    
    \item \textbf{Decomposition} methods break down a \gls{gnn}'s prediction into contributions from individual nodes or edges by creating score decomposition rules through back-propagation \cite{yuan_explainability_2020}. These rules attribute the prediction scores to individual features in the input space \cite{yuan_explainability_2020, baldassarre_explainability_2019}. Notable decomposition approaches are Layer-wise Relevance Propagation (LRP) \cite{baldassarre_explainability_2019} and \gls{gnn}-LRP \cite{schnake_higher-order_2022}.
    
    \item \textbf{Surrogate} methods create a separate, easy-to-interpret model to approximate the behavior of a \gls{gnn} on data similar to the explained example \cite{yuan_explainability_2020}. Usually, neighboring data to the explained example is sampled and used to train easily analyzable models \cite{huang_graphlime_2023} like linear regression models \cite{duval_graphsvx_2021} or Bayesian networks \cite{vu_pgm-explainer_2020}. Relevant publications that leverage surrogate models for explanations are Probabilistic Graphical Model Explainer (PGM-Explainer) \cite{vu_pgm-explainer_2020} and Graph Local Interpretable Model Explanations (GraphLIME) \cite{huang_graphlime_2023}.
    
    \item \textbf{Perturbation}-based methods analyze how the output of a \gls{gnn} model changes when the input is perturbed \cite{yuan_explainability_2020}. Input perturbations refer to alterations to the input graphs \cite{yuan_explainability_2020}. Perturbation-based methods usually create feature masks that highlight those input features that are important for the prediction and exclude the unimportant features \cite{yuan_explainability_2020, ivanovs_perturbation-based_2021}. Perturbation-based methods operate with either a learning-based or a search-based perturbation approach \cite{xia_explaining_2023}. Popular learning-based perturbation approaches are GNNExplainer \cite{ying_gnnexplainer_2019} and PGExplainer \cite{luo_parameterized_2020}. GNNExplainer \cite{ying_gnnexplainer_2019} uses a trainable model to generate masks for the input features, which requires training for each explained prediction separately. In contrast, PGExplainer \cite{luo_parameterized_2020} uses a trainable masking model that is only trained once for a graph and can then be applied to explain any prediction on that graph. In terms of search-based perturbation approaches, SubgraphX \cite{yuan_explainability_2021} is an influential method that uses Monte Carlo tree search to find perturbations that explain a prediction.
\end{itemize}

While gradient/feature and decomposition approaches require direct access to the internal model parameters or embeddings by design, some surrogate and perturbation-based methods can operate without such access \cite{kakkad_survey_2023}. Needing direct access is a disadvantage since it requires adaptations in the explainer before these methods can be applied to new models. It also prohibits the application of the explainers to models that do not expose their inner workings.

As mentioned description of the explanation mechanisms, perturbation-based methods use either a learning-based or a search-based perturbation approach \cite{xia_explaining_2023}. Both of these approaches usually generate a perturbed subgraph of the original input graph, which serves as an explanation. Learning-based approaches identify the most critical features of the input graph by feeding the node embeddings and/or edge embeddings produced by the explained \gls{gnn} into a learnable model that extracts a perturbed subgraph \cite{kakkad_survey_2023, xia_explaining_2023}. The \gls{gnn} predictions on this subgraph are then compared to the original predictions with a distance function, providing a score that serves as training input for the subgraph extraction model \cite{kakkad_survey_2023}. Search-based perturbation approaches generate modified subgraphs from the original input using heuristic search algorithms together with a game-theoretical scoring function \cite{xia_explaining_2023, yuan_explainability_2021}. A clear advantage of the search-based perturbation strategy is that model access is only needed on the level of predictions, not for the embeddings. This comes at the cost of longer inference times \cite{xia_explaining_2023} since the search-based approach requires the separate exploration of various input perturbations for each explained instance \cite{yuan_explainability_2021}.

% Write something to put these methods into perspective. For example a disctinction between search- and learning-based perturbation approaches should be included since I refer to it in the related work section

% Idea would be to compare these 4 approaches and to discuss their most interesting characteristics and shortcomings. I wonder if it makes sense to actually detail the inner workings of any particular explainer or not because it is only inderectly relevant to my thesis

\subsection{Counterfactual Explanation Approaches}
\label{s_ExplainingGNNs_CounterFactual}

Counterfactual explanations for \glspl{gnn} answer the question, "What changes in the input graph are necessary to change the prediction produced by a \gls{gnn} model?". Counterfactual explanations aim to identify so-called counterfactual examples. A counterfactual example consists of the smallest possible alteration to the input information such that the model prediction changes \cite{kakkad_survey_2023}. These counterfactual examples consist of the input information that is \textit{necessary} to the prediction of the explained model \cite{tan_learning_2022}. If the information contained in a counterfactual example is excluded from the input, the prediction of the explained model changes \cite{tan_learning_2022}. Counterfactual explanations for \glspl{gnn} have risen in attention over the last years \cite{ma_clear_2022} for their ability to produce intuitive explanations \cite{ma_clear_2022}, offer feedback for non-experts \cite{prado-romero_survey_2023}, foster trust in \glspl{gnn} \cite{prado-romero_survey_2023}, and help in addressing model biases \cite{prado-romero_survey_2023}. Further, they are useful for identifying potential adversarial examples that could be used to attack models \cite{lucic_cf-gnnexplainer_2022}.

Typically, counterfactual examples consist of a subgraph $\mathcal{X} \subseteq G$ which if removed from the input graph $G$ will result in the explained \gls{gnn} $f(\cdot)$ producing a different prediction than on the full original graph $G$ \cite{tan_learning_2022}:
\begin{equation}
    \label{e_cf_explanation}
    f(G) \neq f(G\setminus \mathcal{X})
\end{equation}

In some cases, counterfactual examples also include information added to the original graph $G$ \cite{abrate_counterfactual_2021}. In such instances, the definition of counterfactual examples is extended. In this extended definition a counterfactual example $\mathcal{X} = \{\mathcal{X}^-, \mathcal{X}^+\}$ consist of a graph $\mathcal{X}^- \subseteq G$, containing information removed from $G$, and a graph $\mathcal{X}^+ = (V^+, E^+)$ with ${v_i \in V^+ \implies v_i \notin V}$ and $e_i \in E^+ \implies e_i \notin E$ that contains information added to the input graph. This combination must satisfy the following equation:

\begin{equation}
    f(G) \neq f((G \cup \mathcal{X}^+) \setminus \mathcal{X^-})
\end{equation}

All counterfactual explanation methods leverage a perturbation-based approach. Similar to the perturbation approaches for factual explanations, a difference between search-based and learning-based approaches is made.

%Counterfactual explainers fall into three high-level methods that differ in how they seek alterations in the input graph to create counterfactual explanations.

\begin{itemize}
    \item \textbf{Search-based} approaches either explore alterations to the input graph dependent on a specific criterion \cite{prado-romero_survey_2023}, like a similarity measure between separate alterations of a graph \cite{abrate_counterfactual_2021}, or they use a systematic policy for modifying the input graph until they attain a valid counterfactual \cite{prado-romero_survey_2023}. Examples of this type of counterfactual explainers are Molecular Model Agnostic Counterfactual Explanations (MMACE) \cite{wellawatte_model_2022} and Oblivious Backward Search (OBS) \cite{abrate_counterfactual_2021}.
    %\item \textbf{Search based} approaches explore alterations to the input graph dependent on a specific criterion \cite{prado-romero_survey_2023}, like a similarity measure between graph alterations \cite{abrate_counterfactual_2021}, to find a counterfactual example.
    %\item \textbf{Heuristic based} methods depend on a systematic policy for modifying the input graph until they attain a valid counterfactual \cite{prado-romero_survey_2023}. For instance, they may alter the original input by randomly dropping and adding edges until the prediction of the \gls{gnn} changes as done by  \cite{abrate_counterfactual_2021}.
    \item \textbf{Learning based} approaches examine how the \gls{gnn} output changes to variations in the input graph \cite{prado-romero_survey_2023}. They train a model to perturb input graphs in a way that the output of the targeted model changes \cite{lucic_cf-gnnexplainer_2022}. To find counterfactual examples, the trained model is used to produce multiple perturbations of the input graph while checking if the perturbations constitute counterfactual examples \cite{lucic_cf-gnnexplainer_2022}. Various approaches within this category differ in how they create and update the input graph mask and how the model is trained \cite{prado-romero_survey_2023}. A popular representative of this category is CF-GNNExplainer \cite{lucic_cf-gnnexplainer_2022}.
\end{itemize}

There also exist methods that are not fully captured by this classification. For example, RCExplainer \cite{bajaj_robust_2021} combines a learning-based approach with a learned model of the decision logic of a \gls{gnn}.

Counterfactual explainers and \glspl{gnn} are usually decoupled from each other \cite{prado-romero_survey_2023}. Hence, the majority of counterfactual explanation methods for \glspl{gnn} offer the advantage of being model agnostic.

% Make the case for cf explanations over f explanations

\subsection{Comparison of Factual and Counterfactual Approaches}
\label{s_ExplainingGNNs_Comparison}

In comparison, while factual explanations shed light on why a \gls{gnn} makes a particular prediction given the input, counterfactual explanations go a step further by revealing precisely what changes in the input are needed to alter that prediction. Because factual reasoning seeks sufficient information as an explanation for a prediction, it may include redundant information \cite{tan_learning_2022}. In contrast, as counterfactual reasoning only seeks necessary information as explanation, it may only extract a small subset of the information that is actually responsible for the prediction \cite{tan_learning_2022}. For this reason, some works integrate factual and counterfactual reasoning into a single explanation method, for example, CFÂ² \cite{tan_learning_2022}. This has the advantage of addressing the necessity and sufficiency of explanations within the same framework \cite{tan_learning_2022}.

On their own, counterfactual explanations provide a practical and intuitive way for users to grasp what influences the explained model's output in an actionable manner \cite{lucic_cf-gnnexplainer_2022}. By offering concrete, actionable insights into the model's decision-making process \cite{lucic_cf-gnnexplainer_2022}, they empower individuals to understand, trust, and improve \glspl{gnn} in real-world applications \cite{prado-romero_survey_2023}. Thus, counterfactual explanations bridge the gap between the model's black-box behavior and human interpretability, making them a valuable tool for ensuring transparency and accountability.
