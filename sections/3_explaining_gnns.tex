\section{Explaining deep graph models}
\label{s_ExplainingGNNs}

%No longer purely related work. First outline explainable ai in general, then transition to gnns and the cummulate into the next chapter on explanability methods on gnns. Last part of the chapeter is the related work subchapter

% Introduction into related work, laying out the general story of the chapter: From explanations on static graphs and the different approaches to more detailed description of highly related work on explanations on dynamic graphs

Alongside the rise in adaptation of artificial intelligence and deep learning methods over recent years, the idea of explainable artificial intelligence has been gaining increasing attention \cite{adadi_peeking_2018}. This field of research focuses on providing transparency over the mechanisms that underlie artificial models \cite{barredo_arrieta_explainable_2020}. Many machine learning models behave like a black-box, meaning that the internal mechanisms that lead to their predictions remain opaque to their users \cite{prado-romero_survey_2023}. Explainable artificial intelligence differentiates \textit{interpretability}, the degree to which the inner structures of a model are understandable to a human by design \cite{barredo_arrieta_explainable_2020, li_explainability_2022}, from \textit{explainability}, actions taken by a model aiming to explain its internal mechanisms \cite{barredo_arrieta_explainable_2020}. While interpretable models utilize mechanisms that are easy to understand on their own \cite{prado-romero_survey_2023}, black-box models have shown better performance on high-dimensional input data \cite{prado-romero_survey_2023}. Thus, a multitude of approaches that aim to explain such black-box models have been investigated \cite{barredo_arrieta_explainable_2020, adadi_peeking_2018}.

%Transition to explanability for gnns and outline the structure of this chapter.

Even though most works on \glspl{gnn} have only emerged in the past 10 years \cite{wu_comprehensive_2021}, the black-box nature of the majority of these \glspl{gnn} gave rise to a plethora of explanation methods. The proposed approaches vary in many ways, like the type of explanation provided, how the explained model is used, or if and how explainers are trained \cite{kakkad_survey_2023}. Figure \ref{f_taxonomy_explainers} provides a classification of these varied approaches. At the conceptual level, we follow the categorization proposed by Yuan et al. \cite{yuan_explainability_2020}, which distinguishes between model-level and instance-level approaches. Model-level methods explain \glspl{gnn} as a whole, irrespective of any particular input example \cite{yuan_explainability_2020}. Such explanations aim to uncover a high-level understanding of the inner workings of the explained \gls{gnn}. In comparison, instance-level methods explain the model output for a specific input \cite{yuan_explainability_2020}. These explanations are tailored to individual instances, offering a fine-grained understanding of the reasoning behind each prediction.

\begin{figure}[ht]
    \centering
    \include{figures/explainer_taxonomy}
    \caption{Taxonomy of \gls{gnn} explanation methods. Explanation approaches are categorized into instance-level and model-level methods. The category of instance-level methods is further subdivided into factual and counterfactual approaches. Taxonomy adapted from \cite{yuan_explainability_2020}, \cite{prado-romero_survey_2023}, and \cite{kakkad_survey_2023}.}
    \label{f_taxonomy_explainers}
\end{figure}

\textit{Model-level} explainers like XGNN \cite{yuan_xgnn_2020} map the behavior of a \gls{gnn} to patterns in the input graphs \cite{yuan_explainability_2020}. In the generation approach, \glspl{gnn} are explained by training a model to generate graphs that maximize a particular prediction of the \gls{gnn} \cite{yuan_explainability_2020, yuan_xgnn_2020}. The thereby generated graph patterns are then used as an explanation \cite{yuan_xgnn_2020}. While these approaches can provide high-level explanations for the general model behavior, they fail to explain the specifics of why a certain prediction has been made for a given input and are thus not discussed any further.

\textit{Instance-level} approaches are subdivided into those that provide a factual and those that provide a counterfactual explanation. On a fundamental level, factual reasoning inquires, "With A already having occurred, will B occur?" \cite{quelhas_relation_2018, tan_learning_2022}. In contrast, counterfactual reasoning poses the question, "If A did not occur, would B still happen?" \cite{quelhas_relation_2018, tan_learning_2022}. The following sections take a closer look at explanation methods using either type of reasoning.

\subsection{Factual Explanation Approaches}
\label{s_ExplainingGNNs_Factual}

Factual explanations for \glspl{gnn} answer the question: "Given the input A, will the \gls{gnn} predict B?". These explanations seek the input information with the maximum influence on the \gls{gnn}'s prediction \cite{kakkad_survey_2023}. They can take the form of a subgraph $G' \subseteq G$ of the original input $G$ \cite{kakkad_survey_2023} that is considered \textit{sufficient} to produce the original prediction by the \gls{gnn} $f(\cdot)$ \cite{tan_learning_2022}. For a sufficient factual explanation, the following equation holds:

\begin{equation}
    f(G) = f(G')
\end{equation}

Factual explainers for \glspl{gnn} are categorized based on the primary mechanisms they employ to explain predictions, each offering different approaches and forms of explanations:

\begin{itemize}
    \item \textbf{Gradient/Feature} based methods identify important features by analyzing how changes in input features affect the network's predictions \cite{yuan_explainability_2020}. These approaches either employ back-propagation to examine the gradients of the explained prediction concerning the input features or map hidden features through interpolation into the input space \cite{yuan_explainability_2020}. Examples of these methods are Class Activation Mapping (CAM)\cite{pope_explainability_2019} and Sensitivity Analysis (SA) \cite{baldassarre_explainability_2019}.
    \item \textbf{Decomposition} methods break down the \gls{gnn}'s prediction into contributions from individual nodes or edges by creating score decomposition rules through back-propagation \cite{yuan_explainability_2020}. These rules attribute the prediction scores to individual features in the input space \cite{yuan_explainability_2020, baldassarre_explainability_2019}. Notable decomposition approaches are Layer-wise Relevance Propagation (LRP) \cite{baldassarre_explainability_2019} and \gls{gnn}-LRP \cite{schnake_higher-order_2022}.
    \item \textbf{Surrogate} methods create a separate, easy-to-interpret model to approximate the behavior of the \gls{gnn} on data similar to the explained example \cite{yuan_explainability_2020}. Usually, neighboring data to the explained example is sampled and used to train easily analyzable models \cite{huang_graphlime_2023} like linear regression models \cite{duval_graphsvx_2021} or Bayesian networks \cite{vu_pgm-explainer_2020}. Relevant publications that leverage surrogate models for explanations are Probabilistic Graphical Model Explainer (PGM-Explainer) \cite{vu_pgm-explainer_2020} and Graph Local Interpretable Model Explanations (GraphLIME) \cite{huang_graphlime_2023}.
    \item \textbf{Perturbation} based methods analyze how the model output changes when the input is perturbed \cite{yuan_explainability_2020}. These methods usually create feature masks that highlight the input features important for the prediction \cite{yuan_explainability_2020, ivanovs_perturbation-based_2021}. Perturbation-based methods operate with either a learning-based or a search-based perturbation approach \cite{xia_explaining_2023}. Popular learning-based perturbation approaches are GNNExplainer \cite{ying_gnnexplainer_2019}, which requires training for each explained prediction, and PGExplainer \cite{luo_parameterized_2020}, which is trained once for a graph and can then be applied to explain any prediction on that graph. In terms of search-based perturbation approaches, SubgraphX \cite{yuan_explainability_2021} has been influential.
\end{itemize}

While gradient/feature and decomposition approaches require direct access to the internal model parameters or embeddings by design, some surrogate and perturbation-based methods can operate without such access \cite{kakkad_survey_2023}. Needing direct access is a disadvantage since it requires adaptations in the explainer before it can be applied to new models and prohibits the application of the explainers to models that do not expose their inner workings.

As mentioned before, perturbation-based methods use either a learning-based or a search-based perturbation approach \cite{xia_explaining_2023}. Both of these usually generate a perturbed subgraph of the original input, which serves as an explanation. Learning-based approaches identify the most critical features of the input graph by feeding the node embeddings and/or edge embeddings produced by the explained \gls{gnn} into a learnable model that extracts a perturbed subgraph \cite{kakkad_survey_2023, xia_explaining_2023}. The \gls{gnn} predictions on this subgraph are then compared to the original predictions with a distance function, providing a score that serves as training input for the subgraph extraction model \cite{kakkad_survey_2023}. Search-based perturbation approaches generate modified subgraphs from the original input using heuristic search algorithms together with a game-theoretical scoring function \cite{xia_explaining_2023, yuan_explainability_2021}. A clear advantage of the search-based perturbation strategy is that model access is only needed on the level of predictions, not for the embeddings. This comes at the cost of longer inference times \cite{xia_explaining_2023} since the search-based approach requires the explicit exploration of various input perturbations.

% Write something to put these methods into perspective. For example a disctinction between search- and learning-based perturbation approaches should be included since I refer to it in the related work section

% Idea would be to compare these 4 approaches and to discuss their most interesting characteristics and shortcomings. I wonder if it makes sense to actually detail the inner workings of any particular explainer or not because it is only inderectly relevant to my thesis

\subsection{Counterfactual Explanation Approaches}
\label{s_ExplainingGNNs_CounterFactual}

Counterfactual explanations for \glspl{gnn} answer the question, "What changes in the input A are necessary for the \gls{gnn} to predict differently, no longer predicting B?". These explanations consist of the smallest possible alteration to the input information such that the model prediction changes \cite{kakkad_survey_2023}. These counterfactual explanations are often referred to as counterfactual examples and consist of the input information that is \textit{necessary} to the prediction of the explained model \cite{tan_learning_2022}. If the information from the counterfactual explanation is excluded from the input, the prediction of the explained model changes \cite{tan_learning_2022}. Counterfactual explanations for \glspl{gnn} have risen in attention over the last years \cite{ma_clear_2022} for their ability to produce intuitive explanations \cite{ma_clear_2022}, offer feedback for non-experts \cite{prado-romero_survey_2023}, foster trust in \glspl{gnn} \cite{prado-romero_survey_2023}, and help address model biases \cite{prado-romero_survey_2023}. Further, they are useful for identifying potential adversarial examples that could be used to attack models \cite{lucic_cf-gnnexplainer_2022}.

Typically, counterfactual explanations consist of a subgraph $\mathcal{X} \subseteq G$ which if removed from the input graph $G$ will result in the explained \gls{gnn} $f(\cdot)$ producing a different prediction than on the full original graph $G$ \cite{tan_learning_2022}:
\begin{equation}
    \label{e_cf_explanation}
    f(G) \neq f(G\setminus \mathcal{X})
\end{equation}

In some cases, counterfactual examples also include information added to the original graph $G$ \cite{abrate_counterfactual_2021}. In such instances, the definition of counterfactual explanations is extended. In this extended definition counterfactual explanations $\mathcal{X} = \{\mathcal{X}^-, \mathcal{X}^+\}$ consist of a graph $\mathcal{X}^- \subseteq G$, containing information removed from $G$, and a graph $\mathcal{X}^+ = (V^+, E^+)$ with ${v_i \in V^+ \implies v_i \notin V}$ and $e_i \in E^+ \implies e_i \notin E$ that contains information added to the input graph. This combination must satisfy the following equation:

\begin{equation}
    f(G) \neq f((G \cup \mathcal{X}^+) \setminus \mathcal{X^-})
\end{equation}

The counterfactual explanation methods all leverage a perturbation-based approach. Similar to the perturbation approaches for factual explanations, a difference between search-based and learning-based approaches is made.

%Counterfactual explainers fall into three high-level methods that differ in how they seek alterations in the input graph to create counterfactual explanations.

\begin{itemize}
    \item \textbf{Search-based} approaches either explore alterations to the input graph dependent on a specific criterion \cite{prado-romero_survey_2023}, like a similarity measure between graph alterations \cite{abrate_counterfactual_2021}, or they use a systematic policy for modifying the input graph until they attain a valid counterfactual \cite{prado-romero_survey_2023}. Examples of this type of counterfactual explainers are Molecular Model Agnostic Counterfactual Explanations (MMACE) \cite{wellawatte_model_2022} and Oblivious Backward Search (OBS) \cite{abrate_counterfactual_2021}.
    %\item \textbf{Search based} approaches explore alterations to the input graph dependent on a specific criterion \cite{prado-romero_survey_2023}, like a similarity measure between graph alterations \cite{abrate_counterfactual_2021}, to find a counterfactual example.
    %\item \textbf{Heuristic based} methods depend on a systematic policy for modifying the input graph until they attain a valid counterfactual \cite{prado-romero_survey_2023}. For instance, they may alter the original input by randomly dropping and adding edges until the prediction of the \gls{gnn} changes as done by  \cite{abrate_counterfactual_2021}.
    \item \textbf{Learning based} approaches examine how the \gls{gnn} output changes with input variations \cite{prado-romero_survey_2023}. First, a mask to highlight relevant features in the input graph is generated, which is then used to modify the input graph such that important features stay the same \cite{prado-romero_survey_2023}. This modified input graph is then fed into the \gls{gnn} again to get a score for this modification \cite{prado-romero_survey_2023}. The score is then used to update the mask for the next iteration \cite{prado-romero_survey_2023}. Various approaches within this category differ in how they create and update the input graph mask. A popular representative of this category is CF-GNNExplainer \cite{lucic_cf-gnnexplainer_2022}.
\end{itemize}

There also exist methods that are not fully captured by this classification. For example, RCExplainer \cite{bajaj_robust_2021} combines a learning-based approach with a learned model of the decision logic of a \gls{gnn}.

% Make the case for cf explanations over f explanations

\subsection{Comparison of Factual and Counterfactual Approaches}
\label{s_ExplainingGNNs_Comparison}

In comparison, while factual explanations shed light on why a \gls{gnn} makes a particular prediction given the input, counterfactual explanations go a step further by revealing precisely what changes in the input are needed to alter that prediction. Because factual reasoning seeks sufficient information as an explanation for a prediction, it may include redundant information \cite{tan_learning_2022}. In contrast, as counterfactual reasoning only seeks necessary information as in explanation it may only extract a small subset of the information that is actually responsible for the prediction \cite{tan_learning_2022}. For this reason, some works integrate factual and counterfactual reasoning into a single explanation method, as done by CF² \cite{tan_learning_2022}. This has the advantage of addressing the necessity and sufficiency of explanations within the same framework \cite{tan_learning_2022}.

On their own, counterfactual explanations provide a practical and intuitive way for users to grasp what influences the explained model's output in an actionable manner \cite{lucic_cf-gnnexplainer_2022}. By offering concrete, actionable insights into the model's decision-making process \cite{lucic_cf-gnnexplainer_2022}, they empower individuals to understand, trust, and even improve \glspl{gnn} in real-world applications \cite{prado-romero_survey_2023}. Thus, counterfactual explanations bridge the gap between the model's black-box behavior and human interpretability, making them a valuable tool for ensuring transparency and accountability.
