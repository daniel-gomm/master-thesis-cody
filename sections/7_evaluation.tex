\section{Evaluation}
\label{s_Evaluation}
% Decide whether this should be a chapter for the entire evaluation including setup and description of datasets baselines etc. or if this should focus on results and discussion only (then rename and move the rest to the methodology chapter)

This chapter provides a comprehensive evaluation of \gls{greedycf} and \gls{cftgnn}. To assess the performance and effectiveness of both methods in various scenarios, a series of experiments is conducted in different settings and on multiple datasets, comparing the proposed approach against baseline explanation methods. To set the stage, the next chapter outlines the experimental setup (Section \ref{s_Evaluation_Setup}), which is followed by an overview of the results of this evaluation in Section \ref{s_Evaluation_Results}.

\subsection{Experimental Setup}
\label{s_Evaluation_Setup}

To ensure the replicability of the results of the evaluation, this section details the experimental setup used for evaluating \gls{greedycf} and \gls{cftgnn}. First, the datasets are introduced (Section \ref{s_Evaluation_Setup_Datasets}). Then, the explained target model and its configuration are detailed in Section \ref{s_Evaluation_Setup_TargetModel}. Section \ref{s_Evaluation_Setup_Explainers} details the explainers compared in the evaluation. Subsequently, Section \ref{s_Evaluation_Setup_Metrics} outlines the metrics that are used to evaluate and compare the explanation approaches. After a brief discussion of the physical evaluation infrastructure (Section \ref{s_Evaluation_Setup_Infrastructure}), the different experimental settings are outlined in Section \ref{s_Evaluation_Setup_Experiments}.

\subsubsection{Datasets}
\label{s_Evaluation_Setup_Datasets}

The explainers are evaluated on three different datasets. These datasets are diverse in terms of their size, their structure, and the temporal density of events, aiming to verify that the explanation approaches perform similarly on different datasets.

The first two datasets come from an online social network among students of the University of California at Irvine \cite{kunegis_konect_2013}. The first of these is called UCI-Messages. In this dynamic graph, nodes represent students and edges private messages between these students. No node or edge features are included in the dataset. 

UCI-Forums is the second dataset. The nodes in this bipartite dynamic graph represent students and forums. Each edge represents a post that a student made to a specific forum. Neither the nodes nor the edges are associated with features.

The third dataset is called Wikipedia \cite{kumar_predicting_2019} and consists of events representing edits of Wikipedia pages. The edits are represented as attributed links between users and pages. Thus, the dataset is a bipartite graph. It encompasses the 1,000 most edited pages and a total of 8,227 users. The edges are associated with attributes that represent a conversion of the edit text into a \gls{liwc} \cite{pennebaker_linguistic_2001} feature vector. This vector encodes a quantitative analysis of the text in the page edit \cite{pennebaker_linguistic_2001}.

\begin{table}[ht]
    \centering
    \begin{tabular}{cccccc}
        \hline
         Dataset&  \# Nodes&  \# Edges&  \# Unique Edges& Timespan & Graph Type\\
         \hline
         UCI-Messages \cite{kunegis_konect_2013} & 1,899 & 59,835 & 20,296 &196 days & Unipartite\\
        UCI-Forums \cite{kunegis_konect_2013}& 1,421& 33,720& 7,089&165 days& Bipartite\\
        Wikipedia \cite{kumar_predicting_2019}& 9,227& 157,474& 18,257&30 days& Bipartite\\
        \hline
    \end{tabular}
    \caption{Statistics for datasets. Table adapted from \cite{poursafaei_towards_2022}.}
    \label{t_Datasets}
\end{table}

Table \ref{t_Datasets} provides an overview of these datasets. The two datasets from the UCI social network cover a longer timespan and are more sparse in the temporal dimension than the Wikipedia dataset. Another major difference is that, in contrast to the other datasets, the UCI-Messages dataset contains a relatively high number of unique edges compared to the number of total edges. 
Furthermore, the UCI-Messages and UCI-Forums datasets have proportionally fewer multi-edges compared to the Wikipedia dataset.


\subsubsection{Target Model}
\label{s_Evaluation_Setup_TargetModel}

All experiments are conducted on the dynamic graph model \gls{tgn} \cite{rossi_temporal_2020}. \gls{tgn} is a \gls{tgnn} for \glspl{ctdg} that was introduced in Section \ref{s_Background_TGNNs}. \gls{tgn} is used as the target model since it is popular within research \cite{souza_provably_2022} and achieves state-of-the-art performance in different tasks on dynamic graphs \cite{rossi_temporal_2020, souza_provably_2022}. To its users, the model mostly remains a black-box with little transparency over what leads to a specific prediction.

For the experiments, the \gls{tgn} model is configured with a \gls{gru} as memory updater, graph attention as an embedding module, last message aggregation, and the identity memory function. This corresponds to the \gls{tgn}-attn configuration presented as the default configuration by the authors of \gls{tgn} \cite{rossi_temporal_2020}. The batch size is set to 32 events. For training, the data is split into four temporal sections. The first $20\%$ of the data are processed but not used for training. The following $50\%$ of events are used in the training. The remaining $30\%$ are split into equal parts for validation and testing. After training for $30$ epochs with early stopping on the model accuracy, the \gls{tgn} model archives a high average precision, as shown in Table \ref{t_TGNTrainingPerformance}.

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{ccccccc}
    \hline
         Dataset &  \multicolumn{2}{c}{UCI-Messages} & \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         Setting &  Transductive&  Inductive & Transductive&  Inductive&  Transductive& Inductive\\
    \hline
         TGN-attn& 85.86 & 83.26 & 92.72&  89.39&  97.80& 97.39\\
    \hline
    \end{tabular}
    \caption{Average Precision in percent for future link prediction on the UCI-Messages, UCI-Forums, and Wikipedia dataset.}
    \label{t_TGNTrainingPerformance}
\end{table}

\subsubsection{Explainers}
\label{s_Evaluation_Setup_Explainers}
As there are not yet any counterfactual explanation methods for dynamic graph models, \gls{cftgnn} and \gls{greedycf} are evaluated against T-GNNExplainer \cite{xia_explaining_2023} since this explainer also aims to explain predictions on \glspl{ctdg}. TempME \cite{chen_tempme_2023}, the only other explanation method that targets \glspl{ctdg}, is not considered because, as of the moment of writing, the authors have yet to release the code for this explainer. 

Comparing factual and counterfactual explanation methods is a challenge, as these methods produce substantially different explanations \cite{tan_learning_2022}. Consequently, the assessment of explanatory performance is conducted separately for necessity (see Section \ref{s_Evaluation_Results_Neccesity}) and sufficiency aspects (see Section \ref{s_Evaluation_Results_Sufficiency}), allowing for an in-depth investigation into both the factual and counterfactual dimensions of the explanations.

If not explicitly mentioned, all experiments in this evaluation are conducted with the same settings for the different explainers. The settings for the explainers are chosen to allow for a fair comparison between explanation methods while maintaining a reasonable runtime. Furthermore, the parameters for \gls{cftgnn} are not yet optimized. Tuning these parameters is left to future work.

The T-GNNExplainer is trained and configured as described by its authors \cite{xia_explaining_2023}. The rollout number is fixed at $500$ iterations, and the number of candidate events is limited to $30$. The original implementation is used, including all the modifications the authors made to the target model \gls{tgn} that are necessary for T-GNNExplainer to work properly.

For \gls{greedycf}, the upper limit on the candidate events $m_{max}$ is set to $64$. The sample size $l$ is configured to $10$, meaning that up to $10$ events are sampled to expand the search tree in each iteration.

For \gls{cftgnn}, the upper limit on the candidate events $m_{max}$ is also set to $64$. The maximum number of iterations $it_{max}$ is $300$. To balance the exploration and exploitation of the search tree, the hyperparameters $\alpha$ and $\beta$ are set to $\alpha = 2$ and $\beta = 1$.

\subsubsection{Metrics} 
\label{s_Evaluation_Setup_Metrics}
The choice of appropriate metrics is crucial for evaluating the effectiveness of explanation methods, as it allows for a fair and comprehensive assessment of their performance. As discussed in Sectio \ref{s_ProblemFormulation_Objectives}, the primary objectives of the explainer are maximizing discoveries and minimizing complexity. To judge how well \gls{cftgnn} and \gls{greedycf} achieve these objectives, appropriate metrics are required.

The fidelity metric has been proposed by earlier work to evaluate how well explanation methods identify important input features \cite{yuan_explainability_2020}. Prior research has established different definitions for the fidelity metric \cite{yuan_explainability_2020, lucic_cf-gnnexplainer_2022, xia_explaining_2023, amara_graphframex_2022, prado-romero_survey_2023}. Different formulations exist, targeting the explained phenomenon and the explained model \cite{amara_graphframex_2022}. When targeting the phenomenon, fidelity is assessed in relation to the ground-truth concerning the occurrence of the future link; conversely, when targeting the model, fidelity is measured in reference to the prediction of the link prediction function \cite{amara_graphframex_2022}. Since the aim of this thesis is to explain models, not the data, the fidelity metric is calculated in regard to the model. Specifically, this work adapts the scores $fid_-$ and $fid_+$ \cite{amara_graphframex_2022, yuan_explainability_2020} to explanations on dynamic graph models:

\begin{equation}
    fid_- = 1 - \frac{1}{N} \sum_{i = 1}^N \mathbbm{1}(p(f(\mathcal{G}(t_i)), \varepsilon_i) = p(f(\mathcal{X}_{\varepsilon_i}, \varepsilon_i)))
\end{equation}

\begin{equation}
    fid_+ = 1 - \frac{1}{N} \sum_{i = 1}^N \mathbbm{1}(p(f(\mathcal{G}(t_i)), \varepsilon_i) = p(f(\mathcal{G}(t_i) \setminus \mathcal{X}_{\varepsilon_i}, \varepsilon_i))) 
\end{equation}

Here, $\varepsilon_1, ..., \varepsilon_N$ are possible future links and $\mathcal{X}_{\varepsilon_1},...,\mathcal{X}_{\varepsilon_N}$ the associated explanations for their prediction. The indicator function $\mathbbm{1}(a = b)$ returns $1$, if $a$ is equal to $b$, else it return $0$.

The $fid_-$ score measures how well the explanation captures the relevant past events that lead the dynamic graph model to its prediction. Thus, it measures the sufficiency of the explanations and is most useful for judging the quality of factual explanations \cite{amara_graphframex_2022}. The $fid_-$ score can be interpreted as the fraction of explanations that are sufficient on their own to produce the original prediction. More relevant to counterfactual explanations is the $fid_+$ score, which measures whether the prediction changes when the explanation is removed from the set of past events. Hence, it provides insights into the necessity of the explanations \cite{amara_graphframex_2022, tan_learning_2022}. Recalling the definition of a counterfactual example from Section \ref{s_ProblemFormulation_CFExamples}, the $fid_+$ score represents the proportion of cases in which the explainer manages to find a necessary explanation in the form of a counterfactual example. Therefore, the $fid_+$ metric provides a measure for the objective of maximizing discoveries. Both types of fidelity scores take on values between $0$ and $1$, with higher values being preferable over lower ones.

To bring the $fid_+$ and the $fid_-$ score into a single score that characterizes both sufficiency and necessity requirements, the characterization score $char$ proposed by Amara et al. \cite{amara_graphframex_2022} is adopted. It is defined as the weighted harmonic mean of the two fidelity scores:

\begin{equation}
    char = \frac{w_+ + w_-}{\frac{w_+}{fid_+} + \frac{w_-}{fid_-}}
\end{equation}

Where $w_+$ and $w_-$ are weights for $fid_+$ and $fid_-$ that allow putting more attention on either sufficiency or necessity. To make a fair comparison between the counterfactual explainers proposed in this thesis and the factual baseline explainer, the weights are set to $w_+ = w_- = 0.5$. The characterization score $char$ takes on values between $0$ and $1$, where larger values indicate better performance.

The sparsity metric $sparsity$ is a widely used tool to gauge the complexity of explanations \cite{yuan_explainability_2020, amara_graphframex_2022, prado-romero_survey_2023}, which addresses the second objective of minimizing complexity.

\begin{equation}
    sparsity = \frac{1}{N} \sum_{i = 1}^N \frac{|\mathcal{X}_{\varepsilon_i}|}{|C(\mathcal{G}, \varepsilon_i, k, m_{max})|}
\end{equation}

The sparsity metric measures the ratio between the past events in the explanation $\mathcal{X}_{\varepsilon_i}$ and all past events considered as candidates for the explanation $C(\mathcal{G}, \varepsilon_i, k, m_{max})$. It takes on values between $0$ and $1$, with lower values being considered better than higher values.

Finally, the Jaccard similarity \cite{jaccard_distribution_1912} $J(\mathcal{X}_i, \mathcal{X}_j)$ is employed to analyze the similarity between the explanations produced by different explanation methods. The Jaccard similarity of two explanations $\mathcal{X}_i$ and $\mathcal{X}_j$ is defined as:

\begin{equation}
    J(\mathcal{X}_i, \mathcal{X}_j) = \frac{|\mathcal{X}_i \cap \mathcal{X}_j|}{|\mathcal{X}_i \cup \mathcal{X}_j|}
\end{equation}

The Jaccard similarity measures the similarity between two explanations by comparing the intersection of the explanations with the union of the explanations. The intersection of the explanation $\mathcal{X}_i \cap \mathcal{X}_j$ comprises the events included in both explanations, whereas the union $\mathcal{X}_i \cup \mathcal{X}_j$ includes the events that are present in either one or both explanations.

% Oracle calls?

% Runtime/Duration

% AUFSC? Maybe also include

% 

\subsubsection{Infrastructure}
\label{s_Evaluation_Setup_Infrastructure}
The experiments are conducted on a high-performance computing cluster with specific hardware configurations tailored to the datasets used. For the UCI-Forums and UCI-Messages datasets, each experiment is run on a machine with an Intel Xeon Gold 6230 CPU, 16GB of RAM storage, and an NVIDIA Tesla V100 SXM2 GPU with 32GB of VRAM. Similarly, for the Wikipedia dataset, each experiment is conducted on a machine with an Intel Xeon Platinum 8358 CPU, 16GB of RAM storage, and an NVIDIA Tesla A100 GPU with 80GB of VRAM.

\subsubsection{Experiments}
\label{s_Evaluation_Setup_Experiments}

Experiments are conducted for the explanation of predictions on the UCI and Wikipedia datasets. Evaluating explanation methods on a set of mostly correct predictions introduces a bias in the evaluation \cite{amara_graphframex_2022}. Thus, the explanation approaches are evaluated on wrong and correct predictions separately. Each experiment consists of explaining $200$ predictions using one of the explanation methods. The explained predictions are the same across the evaluated explanation approaches. Both \gls{greedycf} and \gls{cftgnn} are evaluated with the selection strategies \textit{random}, \textit{temporal}, \textit{spatio-temporal}, and \textit{1-delta} (see Section \ref{s_Methodology_SelectionStrategies}) separately. When discussing the results for one of the explanation methods achieved in combination with a selection strategy, the combination is referred to as "Explainer-\textit{Selection-Strategy}", for example, \gls{cftgnn}-\textit{temporal} for the \gls{cftgnn} explainer paired with the \textit{temporal} selection strategy.

\FloatBarrier
\subsection{Results}
\label{s_Evaluation_Results}

This section presents and discusses the results of the evaluation. To take a look at the explanatory capabilities of the compared explainers, the produced explanations are first evaluated in terms of necessity (Section \ref{s_Evaluation_Results_Neccesity}), then in terms of their sufficiency (Section \ref{s_Evaluation_Results_Sufficiency}), before taking a convergent perspective onto both the necessity and sufficiency of the explanations (Section \ref{s_Evaluation_Results_ConvergentAnalysis}). After the analysis of the explanatory performance, other aspects of the explanation approaches are compared. Section \ref{s_Evaluation_Results_Runtime} takes a look at the runtime of the approaches. In Section \ref{s_Evaluation_Results_Iterations}, the influence of search iterations on the performance of \gls{cftgnn} is evaluated in detail. Next, Section \ref{s_Evaluation_Results_Similarities} presents and discusses the degree of similarity among the explanations produced by the evaluated explainers. Finally, Section \ref{s_Evaluation_Results_SelectionStrategies} studies the effects of the selection strategy on the \gls{greedycf} and \gls{cftgnn} approaches.


 
\FloatBarrier
\subsubsection{Neccesity of Explanations}
\label{s_Evaluation_Results_Neccesity}

Counterfactual examples include information necessary for the original prediction of the target model. As previously discussed, the $fid_+$ score provides a measure for the degree to which an explainer discovers counterfactual explanations, which captures the necessity. Table \ref{t_fid_plus} provides an overview of the $fid_+$ scores of all the evaluated explanation approaches. In all but one scenario, the \gls{cftgnn} approach with either the \textit{spatio-temporal} or the \textit{1-delta} selection strategies performs best. There exists a pattern that \gls{cftgnn}-\textit{spatio-temporal} performs better than \gls{cftgnn}-\textit{1-delta} when explaining correct predictions on any dataset, while \gls{cftgnn}-\textit{1-delta} outperforms \gls{cftgnn}-\textit{spatio-temporal} whenever the focus is on wrong predictions. \gls{greedycf}-\textit{spatio-temporal} performs best for correct predictions on the UCI-Messages and the UCI-Forums datasets. The \textit{random} selection strategy is outperformed by the other selection strategies for both \gls{greedycf} and \gls{cftgnn} across all datasets, with the exception of \gls{cftgnn}-\textit{temporal}, which performs slightly worse than \gls{cftgnn}-\textit{random} for correct predictions on the Wikipedia dataset. This suggests that the selection strategies \textit{temporal}, \textit{spatio-temporal}, and \textit{1-delta} aid \gls{greedycf} and \gls{cftgnn} in finding necessary explanations. The advantage in terms of $fid_+$ scores achieved by these selection strategies over the \textit{random} selection strategy is particularly pronounced for \gls{greedycf}.


%\subsubsection{Explanation Performance}
%\label{s_Evaluation_Results_Performance}

%This section covers the performance of the evaluated explanation methods for the objectives of maximizing discoveries and minimizing complexity. The main aim of the proposed explanation methods is to provide counterfactual explanations. To generate counterfactual examples with the T-GNNExplainer approach, the subgraph it generates is removed and interpreted as a counterfactual example. As discussed before, the $fid_+$ score provides a good measure of how well an explainer maximizes discovering counterfactual explanations. Table \ref{t_fid_plus} provides an overview of all the evaluated explanation approaches. The \gls{cftgnn} approach with the \textit{1-delta} selection strategy clearly outperforms the other approaches in all but one scenarios. The \textit{temporal} selection strategy is the second-best selection strategy for \glspl{cftgnn}. For the \gls{greedycf} approach, the \textit{temporal} selection strategy performs best in all but one settings. Here the \textit{1-delta} selection strategy is second-best. Across \gls{cftgnn} and \gls{greedycf} the \textit{spatio-temporal} selection strategy performs worst.

\begin{table}
    \centering
    \small
    \begin{tabular}{lcccccc}
    \hline
         &  \multicolumn{2}{c}{UCI-Messages}&  \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         &  Correct&  Wrong&  Correct&  Wrong&  Correct& Wrong\\
         \hline
         T-GNNExplainer&  $0.10$&  $0.25$&  $0.05$&  $0.28$&  $0.05$& $0.22$\\
         \gls{greedycf}-\textit{random}&  $0.02$&  $0.08$&  $0.05$&  $0.07$&  $0.05$& $0.11$\\
         \gls{greedycf}-\textit{temporal}&  $0.14$&  $0.33$&  $0.43$&  $0.31$&  $0.10$& $0.30$\\
         \gls{greedycf}-\textit{spatio-temporal}&  $\textbf{0.20}$&  $0.38$&  $\textbf{0.46}$&  $0.30$&  $0.14$& $0.39$\\
         \gls{greedycf}-\textit{1-delta}&  $0.11$&  $0.35$&  $0.29$&  $0.27$&  $0.08$& $0.28$\\
         \gls{cftgnn}-\textit{random}&  $0.11$&  $0.36$&  $0.32$&  $0.31$&  $0.14$& $0.43$\\
         \gls{cftgnn}-\textit{temporal}&  $0.14$&  $0.39$&  $0.39$&  \underline{$0.38$}&  $0.13$& $0.48$\\
         \gls{cftgnn}-\textit{spatio-temporal}&  $\textbf{0.20}$&  \underline{$0.41$}&  \underline{$0.44$}&  $0.36$&  $\textbf{0.18}$& \underline{$0.53$}\\
 \gls{cftgnn}-\textit{1-delta}& $0.17$& $\textbf{0.42}$& $0.40$& $\textbf{0.41}$& \underline{$0.16$}&$\textbf{0.54}$\\
 \hline
    \end{tabular}
    \caption{$fid_+$ scores of the different explanation methods for explaining wrong and correct predictions. The best result is \textbf{bold}, and the second best is \underline{underlined}. If multiple explainers have the best score, they all are \textbf{bold}.}
    \label{t_fid_plus}
\end{table}


Looking at the $fid_+$ metric in isolation ignores the objective of minimizing complexity. Thus, the sparsity of the explanations is considered as well. Table \ref{t_spar_nec} shows the sparsity over the explanations that are necessary to the explained predictions. Looking at the explanations that are necessary on their own allows for a separate analysis of the necessary explanations. The table shows that both \gls{greedycf} and \gls{cftgnn} produce substantially more concise necessary explanations than T-GNNExplainer. However, looking at these sparsity values without any reference to the $fid_+$ metric can be misleading. For instance, \gls{greedycf}-\textit{random} achieves very low $sparsity$ scores in many settings, however, it only also achieves a substantially lower $fid_+$ score than the other \gls{greedycf} variants. To allow for a more nuanced assessment, Figure \ref{f_fid_spar} shows what $fid_+$ scores the explanation approaches achieve at different sparsity levels. The plots report the cumulative distribution of necessary explanations, e.g., counterfactual examples, over different $sparsity$ levels. Higher $fid_+$ values are considered better, while a low $sparsity$ is preferable. Across all datasets and settings, the T-GNNExplainer approach consistently performs worse than all combinations of \gls{cftgnn} and any selection strategy. This is evident because the $fid_+$ level of the T-GNNExplainer approach is below that of \gls{cftgnn} from low sparsity values to high sparsity values. Similarly, \gls{greedycf} with all but the \textit{random} selection strategy outperforms T-GNNExplainer in most cases, with the exception of the experiment for wrong predictions on the UCI-Forums dataset, where T-GNNExplainer slightly outperforms \gls{greedycf}-\textit{1-delta} for very high sparsity values.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lcccccc}
    \hline
         &  \multicolumn{2}{c}{UCI-Messages}&  \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         &  Correct&  Wrong&  Correct&  Wrong&  Correct& Wrong\\
         \hline
         T-GNNExplainer&  $0.43$&  $0.35$&  $0.29$&  $0.35$&  $0.33$& $0.43$\\
         \gls{greedycf}-\textit{random}&  $0.03$&  $0.02$&  $0.02$&  $0.02$&  $0.04$& $0.02$\\
         \gls{greedycf}-\textit{temporal}&  $0.05$&  $0.04$&  $0.05$&  $0.03$&  $0.06$& $0.03$\\
         \gls{greedycf}-\textit{spatio-temporal}&  $0.08$&  $0.04$&  $0.05$&  $0.04$&  $0.07$& $0.04$\\
         \gls{greedycf}-\textit{1-delta}&  $0.06$&  $0.03$&  $0.04$&  $0.03$&  $0.05$& $0.02$\\
         \gls{cftgnn}-\textit{random}&  $0.08$&  $0.05$&  $0.09$&  $0.06$&  $0.07$& $0.07$\\
         \gls{cftgnn}-\textit{temporal}&  $0.07$&  $0.04$&  $0.10$&  $0.05$&  $0.06$& $0.06$\\
         \gls{cftgnn}-\textit{spatio-temporal}&  $0.06$&  $0.04$&  $0.08$&  $0.04$&  $0.07$& $0.05$\\
 \gls{cftgnn}-\textit{1-delta}& $0.07$& $0.05$& $0.09$& $0.06$& $0.07$&$0.06$\\
 \hline
    \end{tabular}
    \caption{$sparsity$ values when only considering necessary explanations.}
    \label{t_spar}
\end{table}

There is a substantial difference between the explanations for correct predictions and those for wrong predictions for the UCI-Messages and the Wikipedia datasets.
%Having a look at the differences between explaining correct predictions and explaining wrong predictions, the results for the UCI-Messages and the Wikipedia datasets differ substantially. 
For wrong predictions, necessary explanations are found in more than double the cases than for correct predictions in both datasets. The reasons for this phenomenon may lie in what essentially differentiates correct from wrong predictions: For wrong predictions, the past information was misinterpreted by the link prediction function. For correct predictions, the past information was correctly interpreted. Thus, explaining wrong predictions with counterfactuals entails finding the past information that mislead the link prediction function. In contrast, finding a counterfactual example for a correct prediction requires removing information so that the prediction becomes incorrect, which could be challenging if nearly all past information is in line with the prediction. In contrast, finding misleading information could be an easier task. Regardless of the reasons for this difference in $fid_+$ levels, explaining wrong predictions is argued to be more informative than explaining correct predictions because explanations for correct predictions can be biased, as the explanations explain not only the model but also the phenomenon \cite{amara_graphframex_2022}. In contrast, wrong predictions solely explain the model \cite{amara_graphframex_2022}.

\begin{figure}[ht]
    \centering
    \include{figures/evaluation/fid_spar}
    \caption{$fid_+$-$sparsity$ curves on the datasets for the different explanation methods.}
    \label{f_fid_spar}
\end{figure}

While \gls{cftgnn} with either the \textit{1-delta} or the \textit{spatio-temporal} selection strategy performs best in most of the settings, for correct predictions on the UCI-Forums dataset, it is outperformed by \gls{greedycf}-\textit{spatio-temporal}. The reasons for this are unclear. It could be a statistical anomaly that appears on the specific sample that was drawn for the experiment. Section \ref{s_Evaluation_Results_Iterations} shows that \gls{cftgnn} can outperform \gls{greedycf}-\textit{spatio-temporal} for this setting when the number of iterations is increased.


\FloatBarrier

\subsubsection{Sufficiency of Explanations}
\label{s_Evaluation_Results_Sufficiency}

 While the aim of the proposed explanation methods is to produce counterfactual explanations, which do not explicitly aim to achieve sufficiency, this section assesses how the different approaches perform in terms of sufficiency. As discussed before, the $fid_-$ metric is used to analyze the fraction of explanations that achieve sufficiency. For this metric, the explanations are interpreted as factual rather than counterfactual explanations. Table \ref{t_fid_minus} shows how the different approaches perform in this metric. Since T-GNNExplainer is developed to provide factual explanations, it would be of no surprise if it outperformed the other methods in terms of $fid_-$. While this is the case for correct predictions on the UCI-Messages dataset, there are other explainers that perform best in the remaining settings. A potential explanation for this may lie in the implementation of T-GNNExplainer. For the evaluation, the original implementation of T-GNNExplainers was used, as proposed by its authors \cite{xia_explaining_2023}. This implementation uses an approximation strategy for the results of the link prediction function. For this approximation, the past events are restricted to those events that are in a $k$-hop-neighborhood of the explained event, with $k$ set to the number of \gls{gnn} layers if the explained link prediction function is a \gls{tgnn}. This means that all events outside this neighborhood have no influence on the model output. Furthermore, events are submitted to the link prediction function at once. However, the targeted \gls{tgn} model is trained and usually used in a batch setting \cite{rossi_temporal_2020}. This may introduce further errors in the approximation. Overall, the approximation may lead to predictions that differ substantially from the predictions without such approximation. This may partly explain why the $fid_-$ score is rather low across the experiments.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lcccccc}
    \hline
         &  \multicolumn{2}{c}{UCI-Messages}&  \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         &  Correct&  Wrong&  Correct&  Wrong&  Correct& Wrong\\
         \hline
         T-GNNExplainer&  $\textbf{0.82}$&  $0.83$&  $0.57$&  $0.72$&  \underline{$0.90$}& $0.69$\\
         \gls{greedycf}-\textit{random}&  $0.34$&  $0.97$&  $0.28$&  $\textbf{0.99}$&  $0.57$& $\textbf{0.93}$\\
         \gls{greedycf}-\textit{temporal}&  $0.56$&  $\textbf{0.98}$&  $0.61$&  $0.98$&  $0.77$& $0.90$\\
         \gls{greedycf}-\textit{spatio-temporal}&  $0.70$&  $0.96$&  $0.64$&  $0.95$&  $0.82$& $0.88$\\
         \gls{greedycf}-\textit{1-delta}&  $0.67$&  $\textbf{0.98}$&  $0.64$&  $\textbf{0.99}$&  $0.72$& $0.90$\\
         \gls{cftgnn}-\textit{random}&  $0.68$&  $0.95$&  $0.66$&  $0.97$&  $0.88$& $0.88$\\
         \gls{cftgnn}-\textit{temporal}&  $0.69$&  $0.96$&  $0.66$&  $0.98$&  $0.88$& $0.88$\\
         \gls{cftgnn}-\textit{spatio-temporal}&  \underline{$0.72$}&  $0.95$&  $\textbf{0.69}$&  $0.93$&  $\textbf{0.91}$& $0.86$\\
 \gls{cftgnn}-\textit{1-delta}& $0.70$& $0.96$& \underline{$0.68$}& $0.95$& $0.88$&\underline{$0.90$}\\
 \hline
    \end{tabular}
    \caption{$fid_-$ scores of the different explanation methods for explaining wrong and correct predictions. The best result is \textbf{bold}, and the second best is \underline{underlined}. If multiple explainers have the best score, they all are \textbf{bold}.}
    \label{t_fid_minus}
\end{table}

Interestingly, there is an apparent difference between correct and wrong predictions for the performance of \gls{cftgnn} and \gls{greedycf}. The pairings of \gls{cftgnn} with the different selection strategies outperform their respective \gls{greedycf} counterparts by between $3.5\%$ and $57.6\%$ for correct predictions. However, the \gls{greedycf} approaches outperform the respective configurations of \gls{cftgnn} by up to $5.4\%$ for wrong predictions. Generally, the $fid_-$ scores achieved by \gls{cftgnn} and \gls{greedycf} for wrong predictions are very high, reaching levels of more than $0.85$ across the datasets and explainer variants. This underlines the proficiency of these explanation methods in explaining wrong predictions and suggests that counterfactual explanations are associated with a high sufficiency for wrong predictions.
%Interestingly, there is an apparent difference between correct and wrong predictions for the performance of \gls{cftgnn} and \gls{greedycf}. The pairings of \gls{cftgnn} with the different selection strategies outperform their respective \gls{greedycf} counterparts by between $3.5\%$ and $57.6\%$ for correct predictions. However, the \gls{greedycf} approaches outperform the respective configurations of \gls{cftgnn} by up to $5.4\%$.
%Interestingly, there is an apparent difference between correct and wrong predictions for the performance of \gls{cftgnn} and \gls{greedycf}. The pairings of \gls{cftgnn} with the different selection strategies outperform their respective \gls{greedycf} counterparts by between $3.5\%$ and $57.6\%$ for correct predictions. However, the \gls{greedycf} approaches outperform the respective configurations of \gls{cftgnn} by up to $5.4\%$.



\FloatBarrier
\subsubsection{Convergent Analysis of Sufficiency and Necessity}
\label{s_Evaluation_Results_ConvergentAnalysis}

The characterization score $char$ integrates the perspectives of sufficiency and necessity. Table \ref{t_char} shows the performance of the explainers along this metric. While \gls{cftgnn}-\textit{1-delta} performs best in explaining wrong predictions, \gls{cftgnn}-\textit{spatio-temporal} performs best in explaining correct predictions. This convergent perspective is also portrayed in Figure \ref{f_fid_plus_minus}, which shows the performance of the explainers in both the $fid_+$ and the $fid_-$ scores. The plots show that \gls{cftgnn}-\textit{1-delta} and \gls{cftgnn}-\textit{spatio-temporal} generally provide the best explanations in terms of necessity while also providing comparably good performance in terms of sufficiency. 


\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lcccccc}
    \hline
         &  \multicolumn{2}{c}{UCI-Messages}&  \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         &  Correct&  Wrong&  Correct&  Wrong&  Correct& Wrong\\
         \hline
         T-GNNExplainer&  $0.17$&  $0.39$&  $0.08$&  $0.40$&  $0.09$& $0.34$\\
         \gls{greedycf}-\textit{random}&  $0.04$&  $0.14$&  $0.08$&  $0.12$&  $0.09$& $0.19$\\
         \gls{greedycf}-\textit{temporal}&  $0.22$&  $0.49$&  $0.50$&  $0.47$&  $0.17$& $0.45$\\
         \gls{greedycf}-\textit{spatio-temporal}&  \underline{$0.31$}&  $0.54$&  \underline{$0.53$}&  $0.46$&  $0.23$& $0.54$\\
         \gls{greedycf}-\textit{1-delta}&  $0.18$&  $0.51$&  $0.39$&  $0.42$&  $0.14$& $0.43$\\
         \gls{cftgnn}-\textit{random}&  $0.19$&  $0.52$&  $0.43$&  $0.47$&  $0.24$& $0.58$\\
         \gls{cftgnn}-\textit{temporal}&  $0.23$&  $0.55$&  $0.49$&  \underline{$0.54$}&  $0.22$& $0.62$\\
         \gls{cftgnn}-\textit{spatio-temporal}&  $\textbf{0.31}$&  \underline{$0.57$}&  $\textbf{0.54}$&  $0.52$&  $\textbf{0.30}$& \underline{$0.65$}\\
 \gls{cftgnn}-\textit{1-delta}& $0.27$& $\textbf{0.58}$& $0.50$& $\textbf{0.57}$& \underline{$0.27$}&$\textbf{0.68}$\\
 \hline
    \end{tabular}
    \caption{$char$ scores of the different explanation methods for explaining wrong and correct predictions. The best result is \textbf{bold}, and the second best is \underline{underlined}.}
    \label{t_char}
\end{table}


\begin{figure}[ht]
    \centering
    \include{figures/evaluation/fid_plus_minus}
    \caption{$fid_+$ in relation to the $fid_-$ achieved by the explanation approaches.}
    \label{f_fid_plus_minus}
\end{figure}


\FloatBarrier
\subsubsection{Study of Runtime}
\label{s_Evaluation_Results_Runtime}

Figure \ref{f_duration} shows the average time it takes the explanation approaches to explain a prediction. Across all datasets, \gls{greedycf} took the least time on average. Within the variants of \gls{greedycf}, the \textit{1-delta} selection strategy entails a considerably longer time for each explanation. The reason for this difference is that the link prediction function is called a lot more often by \gls{greedycf}-\textit{1-delta} compared to the other variants of \gls{greedycf}. This can be seen in Table \ref{t_oracle_calls}, which shows the average number of calls the evaluated explainers make to the link prediction function. The table shows that, in general, the link prediction function is called a lot more often by \gls{cftgnn} compared to \gls{greedycf}. This is not too surprising, as \gls{greedycf} concludes upon encountering a counterfactual example or when it cannot greedily advance in its search, whereas \gls{cftgnn} only concludes after the predefined number of iterations or when the search tree is already fully explored.

\begin{figure}
    \centering
    \include{figures/evaluation/duration}
    \caption{Average duration of explaining a prediction for the different datasets.}
    \label{f_duration}
\end{figure}


Across all datasets and selection strategies, \gls{cftgnn} spends $97.85\%$ of the explanation duration on calling the link prediction function, whereas \gls{greedycf} spends $99.83\%$, and T-GNNExplainer $82.72\%$ on this. This shows that the main contributor to the runtime is the calls to the link prediction model. Therefore, using a faster model would also speed up explanation times. In contrast to the two explanation methods developed in this thesis, T-GNNExplainer spends considerably longer on the search procedure itself compared to calling the model.

As shown in Table \ref{t_oracle_calls}, the number of calls to the model that are performed by the different explanation approaches is relatively stable across the datasets. However, comparing the runtime of \gls{cftgnn} and \gls{greedycf} across the different datasets shows a large variance. The reason that these approaches take more time on the Wikipedia dataset compared to the other two datasets lies in the longer time it takes to get a response from the targeted model. The cause of this could lie in the fact that the Wikipedia dataset contains edge features, whereas the other two datasets do not. However, the fact that the T-GNNExplainer approach does not show this effect suggests that there may be a different cause. Another difference that could account for the difference is that the Wikipedia dataset contains substantially more edges than the other datasets.

In general, \gls{greedycf} achieves relatively short explanation durations. This makes it the preferable method if fast explanations are a priority.

\begin{table}
    \centering
    \small
    \begin{tabular}{lcccccc}
    \hline
         &  \multicolumn{2}{c}{UCI-Messages}&  \multicolumn{2}{c}{UCI-Forums}&  \multicolumn{2}{c}{Wikipedia}\\
         &  Correct&  Wrong&  Correct&  Wrong&  Correct& Wrong\\
         \hline
         \gls{greedycf}-\textit{random}&  $27.10$&  $23.40$&  $24.80$&  $21.80$&  $23.15$& $18.55$\\
         \gls{greedycf}-\textit{temporal}&  $43.70$&  $29.85$&  $36.15$&  $29.15$&  $32.30$& $24.65$\\
         \gls{greedycf}-\textit{spatio-temporal}&  $57.60$&  $29.40$&  $37.65$&  $35.25$&  $39.35$& $22.35$\\
         \gls{greedycf}-\textit{1-delta}&  $100.15$&  $79.18$&  $88.10$&  $78.15$&  $80.47$& $69.36$\\
         \gls{cftgnn}-\textit{random}&  $287.91$&  $246.83$&  $261.13$&  $257.23$&  $284.52$& $237.16$\\
         \gls{cftgnn}-\textit{temporal}&  $287.92$&  $245.64$&  $261.23$&  $256.31$&  $284.85$& $236.26$\\
         \gls{cftgnn}-\textit{spatio-temporal}&  $287.95$&  $245.44$&  $260.95$&  $256.44$&  $284.98$& $236.26$\\
 \gls{cftgnn}-\textit{1-delta}& $346.50$& $292.23$& $312.00$& $305.70$& $339.90$&$272.52$\\
 \hline
    \end{tabular}
    \caption{Average number of calls to the link prediction function performed by the different explanation methods.}
    \label{t_oracle_calls}
\end{table}

\FloatBarrier
\subsubsection{Study of Search Iterations}
\label{s_Evaluation_Results_Iterations}

One of the parameters of \gls{cftgnn} is the maximum number of search iterations $it_{max}$. Throughout the experiments, this number is set to $300$. To investigate the performance impact of the number of search iterations, another experiment is conducted in which the maximum number of search iterations is raised to $it_{max}=1200$. The experiments are conducted for correct prediction on the Wikipedia dataset because this is the only setting in which \gls{greedycf}-\textit{spatio-temporal} outperforms all \gls{cftgnn} variants. Figure \ref{f_fid_iteration} shows the $fid_+$ score that is achieved in relation to the search iterations. This means it represents the $fid_+$ score that would be achieved if the maximum number of iterations was fixed to that of the number of iterations on the x-axis. The $fid_+$ scored for the \gls{greedycf} variants are independent of the iterations variable because this explainer does not have a configurable maximum number of search iterations. 
The figure demonstrates that all \gls{cftgnn} variants outperform their respective \gls{greedycf} counterparts at some number of maximum iterations. The number of iterations they take varies significantly, while for the \textit{random} and \textit{1-delta} selection strategies a few iterations suffice to surpass their \gls{greedycf} counterparts, it takes almost $1000$ iterations for \gls{cftgnn}-\textit{spatio-temporal} and almost $1200$ for \gls{cftgnn}-\textit{temporal}. 

\begin{figure}[ht]
    \centering
    \include{figures/evaluation/fid_iteration_uci_correct}
    \caption{$fid_+$ scores achieved by the different explanation methods for correct predictions on the Wikipedia dataset. The $fid_+$ scores of the \gls{cftgnn} variants are depicted in relation to the number of search iterations.}
    \label{f_fid_iteration}
\end{figure}

Regardless of the selection strategy, \gls{cftgnn} is guaranteed to find the minimal counterfactual example when given an unlimited number of iterations. That is because with an unlimited number of iterations, \gls{cftgnn} traverses the entire search space, which means that it encounters all counterfactual examples that exist for any prediction instance. The fact that it takes \gls{cftgnn}-\textit{spatio-temporal} and \gls{cftgnn}-\textit{temporal} so many iterations to outperform their \gls{greedycf} counterparts suggests that the hyperparameters $\alpha$ and $\beta$ that influence the balance between exploration and exploitation require optimization. This conclusion is evident because \gls{greedycf} outperforms these \gls{cftgnn} variants up to a high number of iterations, which means that \gls{greedycf} explores parts of the search space that are not explored by \gls{cftgnn}. Since \gls{greedycf} constructs a substantially smaller partial search tree than \gls{cftgnn}, the \gls{cftgnn} variants should be able to construct a partial search tree that includes that of \gls{greedycf}. Since it evidently takes many iterations until it does so, the search may tend to explore or exploit the partial search tree too much instead.



\FloatBarrier
\subsubsection{Study of Similarities in Explanations}
\label{s_Evaluation_Results_Similarities}

The similarities of explanations are studied using the Jaccard similarity between the explanations produced by the different explainers. Since the similarity results are mostly alike across the different experimental settings, Figure \ref{f_jaccard_similarity_wiki} only shows a heatmap of the Jaccard similarities between the explanations for correct predictions on the Wikipedia dataset. The results for all settings are presented in detail in Appendix \ref{s_Appendix_JaccardSimilarities}. The heatmap shows the Jaccard similarity between an explainer denoted in the column and another explainer denoted in the row.

Figure \ref{f_jaccard_similarity_wiki} shows that there is a very low similarity between the explanations produced by T-GNNExplainer and the other approaches. This result suggests that the factual explanations that T-GNNExplainer produces are substantially different from the counterfactual explanations produced by \gls{greedycf} and \gls{cftgnn}. A contributing factor to these low similarity scores is also that the explanations produced by T-GNNExplainer are generally much less sparse than those produced by the other explainers.

When comparing the different \gls{greedycf} variants, a particularly high similarity exists between the explanations of \gls{greedycf}-\textit{temporal} and \gls{greedycf}-\textit{spatio-temporal}. Further, the explanations produced by \gls{greedycf}-\textit{spatio-temporal} are most similar to those produced by \gls{cftgnn} with any selection strategy.

The explanations produced by the different \gls{cftgnn} variants are generally more similar to each other than those of the \gls{greedycf} variants. A particularly high similarity exists between the explanations of \gls{cftgnn}-\textit{temporal} and \gls{cftgnn}-\textit{spatio-temporal}. Furthermore, \gls{cftgnn}-\textit{temporal} and \gls{cftgnn}-\textit{spatio-temporal} also achieve a high similarity with \gls{cftgnn}-\textit{1-delta}. This suggests that the flexibility that the search procedure of \gls{cftgnn} provides, allows \gls{cftgnn} to explore similar parts of the search space regardless of the selection strategy. It also highlights the similarities between the \textit{temporal}, \textit{spatio-temporal}, and \textit{1-delta} selection strategies, as these are associated with explanations that are more similar compared to the \textit{random} selection strategy.


\begin{figure}[ht]
    \centering
    \include{figures/evaluation/similarity/jaccard_similarity_wiki}
    \caption{Jaccard similarity between explanations of correct predictions on the Wikipedia dataset.}
    \label{f_jaccard_similarity_wiki}
\end{figure}


\FloatBarrier

\subsubsection{Study of Selection Strategies}
\label{s_Evaluation_Results_SelectionStrategies}

There exist substantial differences between the performance of \gls{greedycf} and \gls{cftgnn} depending on the selection strategy. This indicates that selecting events for the perturbations based on these strategies influences the operations of the explanation approaches. If there were no effect of a strategy, the explainers would be assumed to perform similarly to the \textit{random} selection strategy. However, since the results for the metrics of $fid_+$ and $fid_-$ show that for both \gls{cftgnn} and \gls{greedycf}, the \textit{random} selection strategy is mostly outperformed by the other selection strategies. This suggests that the selection strategies aid the explainers in exploring relevant areas of the search space.

Taking a look at the results for the $fid_+$ scores achieved by the different \gls{greedycf} variants in Table \ref{t_fid_plus} and Figure \ref{f_fid_spar} clearly shows that the \textit{spatio-temporal} selection strategy is performing best overall. The \textit{temporal} selection strategy performs second best, while the \textit{1-delta} strategy comes in third. This shows that the spatial and temporal proximity of events has a large influence on the prediction of the target model for the investigated datasets. The same ranking of selection strategies also translates to the results on the characterization scores $char$ (see Table \ref{t_char}), indicating that the same findings in terms of an overarching analysis of sufficiency and necessity.


Shifting the view to the results for the $fid_+$ scores achieved by \gls{cftgnn} with the different selection strategies in Table \ref{t_fid_plus} and Figure \ref{f_fid_spar} shows that there is no clear best-performing selection strategy. While for explaining correct predictions, the \textit{spatio-temporal} strategy performs best and the \textit{1-delta} performs second best, it is the other way around for explaining wrong predictions. Across all settings, \gls{cftgnn}-\textit{temporal} generally performs third best in terms of $fid_+$. This ranking of selection strategies based on the necessity of explanations also transfers to a combined assessment of necessity and sufficiency using the characterization score $char$ (see Table \ref{t_char}).

In general, the effects of the selection strategies on the explanatory performance are more pronounced for the \gls{greedycf} approach than for the \gls{cftgnn} approach. The reason for this could lie in the different approaches for exploring the search space that these two methods follow. \gls{cftgnn} is designed to exploit promising paths and to explore further if none of the known paths are promising. Thus, if the selection strategy does not provide good initial selections, it is incentivized to explore the search space in more breadth, compared to \gls{greedycf} for which the breadth of the search is fixed by the sampling parameter $l$.

Across \gls{greedycf} and \gls{cftgnn}, the \textit{spatio-temporal} selection strategy outperforms the \textit{temporal} selection strategy in the majority of experiment settings. This relation evinces that spatial proximity is an important factor for the importance of past events to the prediction outcome of the targeted link prediction model. Thus, taking a combined spatio-temporal perspective on the past serves as a particularly useful method of informing search-based counterfactual explanation methods. Furthermore, the \textit{spatio-temporal} selection strategy has an advantage over the \textit{1-delta} strategy in terms of the time that is required for finding an explanation (see Figure \ref{f_duration}). 
