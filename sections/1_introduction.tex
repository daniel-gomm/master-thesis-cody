\section{Introduction}
\label{s_Introduction}

Recent years have witnessed remarkable strides in artificial intelligence, with a particular surge in the prominence of deep neural networks \cite{prado-romero_survey_2023}. Convolutional Neural Networks have revolutionized image-related tasks \cite{szegedy_going_2015}, and the advent of transformer-based language models \cite{vaswani_attention_2017, brown_language_2020} is redefining how many tasks in natural language processing are performed. These advancements harness the intrinsic patterns within data structures, for example, the grid structure of images. Using these intrinsic patterns allows contemporary deep learning methods to effectively mitigate the challenges posed by an exponential increase in data volume and computational complexity as the number of dimensions in a dataset grows \cite{bronstein_geometric_2017, bronstein_geometric_2021, bellman_dynamic_1966}.
%These advancements harness the intrinsic patterns within data structures, effectively mitigating the challenges posed by the curse of dimensionality \cite{bronstein_geometric_2017, bronstein_geometric_2021}, referring to the exponential increase in data volume and computational complexity as the number of dimensions in a dataset grows \cite{bellman_dynamic_1966}.

%Recent years have seen plenty of advances in artificial intelligence. Especially deep neural networks have found lots of attention and success \cite{prado-romero_survey_2023}. Image-related tasks have been revolutionized by Convolutional Neural Networks \cite{szegedy_going_2015} and transformer-based language models \cite{vaswani_attention_2017, brown_language_2020} are redefining how many tasks in natural language processing are done. These models exploit regularities in the structure of the underlying data to overcome the so-called curse of dimensionality \cite{bronstein_geometric_2017, bronstein_geometric_2021}, referring to the exponential increase in data volume and computational complexity as the number of dimensions in a dataset grows \cite{bellman_dynamic_1966}. 

Research on deep neural networks is advancing to deploy and apply deep learning to data that is structured as graphs \cite{wu_comprehensive_2021}. Deep learning models on graphs follow the success of models for image and text data in exploiting regularities in the structure of the underlying data \cite{bronstein_geometric_2021}.
%Building on the same conceptual blueprint for harnessing the intrinsic patterns \cite{bronstein_geometric_2021}, research on deep neural networks is advancing to develop and apply deep learning approaches to data that is structured as graphs \cite{wu_comprehensive_2021}. 
Such models, which are referred to as \glspl{gnn}, have demonstrated their prowess, for example, contributing to drug discovery through protein folding \cite{jumper_highly_2021} or improving the precision of weather pattern forecasts \cite{lam_graphcast_2023}.
%While \glspl{gnn} have been predominantly applied to graphs that are static and do not change over time, there is a growing body of research that extends the methods of \glspl{gnn} to dynamic graphs \cite{souza_provably_2022}, which are graphs that evolve over time.

%Building on the same conceptual blueprint \cite{bronstein_geometric_2021}, research on deep neural networks has recently been advancing to develop and apply deep learning approaches to data that is structured in the form of graphs \cite{wu_comprehensive_2021}. Such models have been successfully applied to aid in drug discovery through protein folding \cite{jumper_highly_2021} or to accurately forecast weather patterns \cite{lam_graphcast_2023}. These models are referred to as \glspl{gnn}. While \glspl{gnn} have been predominantly applied to graphs that are static and do not change over time, there is a growing body of research that extends the methods of \glspl{gnn} to dynamic graphs \cite{souza_provably_2022}, which are graphs that evolve over time.
While \glspl{gnn} are predominantly applied to graphs that are static and do not change over time, many real-world domains are better described as dynamic graphs. Dynamic graphs are graphs that evolve over time, for instance, social networks, where connections form and dissolve over time \cite{rossi_temporal_2020, souza_provably_2022}, road networks with traffic conditions that constantly change \cite{zhao_t-gcn_2020}, or financial transactions occurring at any given moment. In dynamic graphs, nodes and edges can be added and deleted over time, and their features can evolve. Disregarding the temporal dimension for time-evolving data not only overlooks crucial temporal information for refining the model \cite{xu_inductive_2020} but also risks the model mistakenly using future information to fulfill a task during training or validation, leading to faulty inference \cite{xu_inductive_2020}. Tailoring models specifically for dynamic graphs has proven successful, outperforming their static counterparts in handling dynamic graph data \cite{ma_streaming_2018, makarov_temporal_2021, rossi_temporal_2020, trivedi_dyrep_2019, xu_inductive_2020, souza_provably_2022}. Such models are referred to as \glspl{tgnn}. The dynamic graph data they operate on is usually represented as a sequence of timestamped events that each correspond to an atomic alteration of the graph, like the addition or removal of a new node or edge to the graph.

An example that illustrates the evolving nature of dynamic graphs is financial transaction data. In a financial transaction graph, people are represented as nodes, and transactions between people as timestamped edges between people. In such a graph, the inclusion of a temporal dimension allows answering questions like "Is a potential future transaction likely to happen or not?" with varying answers depending on the current time and state of the graph. Such information can be used as decision support for spotting fraudulent transactions.

While deep models on dynamic graphs are getting increased attention \cite{longa_graph_2023}, they remain black-boxes to their users \cite{vu_limit_2022}. This means that their inputs and outputs are known, but they do not provide insights into why they produce a certain output. Yet, such explanations of the output are crucial in providing reliable and transparent predictions, particularly in critical areas like finance and healthcare \cite{prado-romero_survey_2023}. Regulatory bodies are increasingly recognizing this necessity and have started to mandate explainability for tools that inform decisions in critical fields \cite{european_parliament_proposal_2021}. Existing explanation approaches primarily cater to \glspl{gnn} operating on static graphs \cite{yuan_explainability_2020, kakkad_survey_2023}, lacking any consideration for temporal representation. Without the ability to interpret temporal dependencies in the graph structure, these methods fall short of adequately explaining dynamic graph models \cite{he_explainer_2022, xia_explaining_2023, liu_differential_2023}. Explanation methods for black-box models on dynamic graphs remain understudied \cite{vu_limit_2022, longa_graph_2023}, with only a handful of publications addressing the intricacies of this topic \cite{he_explainer_2022, xia_explaining_2023, xie_explaining_2022, liu_differential_2023, fan_gcn-se_2021}.
%This growing need for explainability is currently not met for \glspl{tgnn}. Existing explanation approaches predominantly cater to \glspl{gnn} operating on static graphs \cite{yuan_explainability_2020, kakkad_survey_2023}, lacking any form of temporal representation. Since such explanation methods have no way of interpreting the temporal dependencies in the graph structure, they lack the ability to explain dynamic graph models properly \cite{he_explainer_2022, xia_explaining_2023, liu_differential_2023}. Explanation methods for black-box models on dynamic graphs are currently understudied \cite{vu_limit_2022, longa_graph_2023} with only a handful of publications that tackle the topic \cite{he_explainer_2022, xia_explaining_2023, xie_explaining_2022, liu_differential_2023, fan_gcn-se_2021}.

Most existing explanation approaches require extensive access to the internals of the temporal models they explain. Only two prior works explain models that support dynamic graphs with a high temporal resolution. Furthermore, all existing methods provide factual explanations. Factual explanations detail the specific nodes, edges, and other features within the input graph that influence and contribute to a particular prediction \cite{tan_learning_2022}. However, they do not explore which changes to the input graph lead to different predictions. Counterfactual explanations, which are an increasingly popular alternative to factual explanations \cite{prado-romero_survey_2023}, address this shortcoming. Counterfactual explanations explain a model's prediction by showing how altering specific elements in the input graph, such as removing or adding nodes or edges, would result in a different prediction outcome \cite{byrne_counterfactuals_2019}. This establishes causal relationships between the data and the model output and outlines decision boundaries \cite{prado-romero_survey_2023}.
%, which means that they explain a model's predictions with the information from the graph that leads the model to make that prediction \cite{tan_learning_2022}. Counterfactual explanations are an increasingly popular alternative to factual explanations for explaining static \glspl{gnn} \cite{prado-romero_survey_2023}. Counterfactual explanations explain a model's prediction by inquiring what changes to the input graph lead to a different prediction of the model, which establishes causal relationships between the data and the model output \cite{byrne_counterfactuals_2019, prado-romero_survey_2023}.
%, and none of them provide counterfactual explanations. Counterfactual explanations are an increasingly popular alternative to classic factual explanations. While factual explanations describe the reasons behind a model's predictions based on actual features and data, counterfactual explanations explore what changes to the input data might alter the model's predictions \cite{lucic_cf-gnnexplainer_2022, tan_learning_2022}.
Counterfactual explanations are particularly useful to users of black-box models because they provide actionable insights into the models' reasoning \cite{lucic_cf-gnnexplainer_2022}, they can be used to identify biases \cite{prado-romero_survey_2023}, and they can help in finding potential adversarial examples \cite{lucic_cf-gnnexplainer_2022}. Despite the utility of counterfactual explanations, none of the existing explanation methods tailored for models on dynamic graphs has harnessed this potential until now.

To fill the gap that currently exists in research, this thesis employs counterfactual explanations to explain black-box models on dynamic graphs. It proposes two novel model-agnostic explanation approaches that provide counterfactual explanations. These approaches use a search-based approach to perturb the input to a target model. For any prediction, the search aims to identify a subset of past events such that removing this subset from the input leads the targeted model to change its prediction. Such a subset is referred to as a counterfactual example. The first search approach is \gls{greedycf}. It leverages a simple greedy approach to the search and serves as a proof-of-concept and as a baseline. The second explanation approach is called \gls{cftgnn}. It uses search principles from reinforcement learning to traverse the search space efficiently. 

An extensive evaluation shows that \gls{cftgnn} is a capable counterfactual explanation method that outperforms \gls{greedycf} and a state-of-the-art factual explainer. Further, the evaluation suggests that informing the proposed explainers with temporal, spatio-temporal, and local gradient information about past events has a significant impact on the search performance.

In summary, the main contributions of this thesis are:

\begin{itemize}
    \item \textbf{Problem Formulation:} This work formulates the problem of counterfactual explanations on dynamic graphs as a search problem. The properties of the search space are outlined, and the search space is structured in a way that facilitates finding explanations with minimal complexity.
    %outlining the properties of the search space and structuring the search space in accordance with minimizing the complexity of counterfactual explanations.

    \item \textbf{Methods:} Two novel counterfactual explainers named \gls{greedycf} and \gls{cftgnn} are proposed to explain predictions of black-box models on dynamic graphs.

    \item \textbf{Experiments:} This thesis proposes a structured approach for conducting a comprehensive evaluation of factual and counterfactual explanation methods in the context of predictions on dynamic graphs. The evaluation shows the superiority of the explanations produced by \gls{cftgnn} over \gls{greedycf} and a state-of-the-art factual explanation method regarding a combined assessment of the sufficiency and necessity.
    
\end{itemize}


This thesis is structured as follows. As this thesis work aims to explain black-box models on dynamic graphs, Section \ref{s_Background} outlines the foundations of static graphs and \glspl{gnn} and how these are extended by a temporal dimension. Subsequently, Section \ref{s_ExplainingGNNs} provides a taxonomic overview of how regular \glspl{gnn} are explained with a focus on the differences between factual and counterfactual explanation methods. Following this, Section \ref{s_RelatedWork} details the current state of research into explanation methods for models on dynamic graphs. It establishes a substantial gap in research that is addressed in this thesis. In the subsequent section (Section \ref{s_ProblemFormulation}), the counterfactual explanation problem for dynamic graphs is formally defined. This problem is addressed by the novel explanation methods \gls{greedycf} and \gls{cftgnn}, which are introduced in Section \ref{s_Methodology}. To establish their superior performance compared to contemporary explanation methods, an extensive evaluation is undertaken in Section \ref{s_Evaluation}. Finally, Section \ref{s_Conclusion} concludes the thesis, discussing the limitations of this work and room for future research.

