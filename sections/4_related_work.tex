\section{Related Work}
\label{s_RelatedWork}

% Argue why we only look at the temporal explanation methods, not at any from the static context. Also need to argue why 

As outlined in Chapter \ref{s_ExplainingGNNs}, numerous explanation methods for \glspl{gnn} on static graphs have been developed. However, these models fall short when applied to temporal graph models, as they fail to capture temporal dependencies in dynamic graphs \cite{xia_explaining_2023, he_explainer_2022}. Additionally, they are unable to support the \gls{dtdg} and \gls{ctdg} representations of dynamic graphs. Particularly, the event-based \gls{ctdg} representation is conceptually substantially different from the static representation of graphs as mere sets of nodes and edges. Yet, only a very limited number of past publications have explored explanation methods that take the dynamic nature of \glspl{tgnn} into account. To develop and evaluate novel explanation methods for \glspl{tgnn}, it is crucial to understand existing explanation methods in the context of dynamic graphs. Hence, this chapter discusses explanation approaches that target models on dynamic graphs introduced by prior works. An overview of explanation methods for \glspl{tgnn} in related works is presented in Figure \ref{f_taxonomy_explainers}. The methods used in these explainers mainly adapt findings from explainers in the static setting to the dynamic context. 

\begin{figure}[ht]
    \centering
    \include{figures/rel_work_overview}
    \label{f_relatedWork_taxonomy}
    \caption{Overview of related work. Some works focus on explaining \glspl{tgnn} operating on the \gls{dtdg} representation, while only two other studies explore explanations for \glspl{tgnn} operating on \glspl{ctdg}. The methods are subdivided by their approach, following the taxonomy introduced for the static explanation approaches in Figure \ref{f_taxonomy_explainers}.}
\end{figure}

This chapter provides a structured overview of existing methods and how they compare to the approaches developed in this thesis. Distinguishing the approaches at the level of the underlying graph representation, the following sections outline related works for explanations in the \gls{dtdg} (Section \ref{s_relatedWork_DTDG}) and the \gls{ctdg} setting (Section \ref{s_relatedWork_CTDG}). Section \ref{s_relateWork_comparison} compares the existing approaches and outlines the gap that exists within contemporary research.

\subsection{Explanations on Discrete Time Dynamic Graphs}
\label{s_relatedWork_DTDG}

Most prior work focuses on explanations for predictions on \glspl{dtdg}. Explaining such models involves integrating feature importances over the different graph snapshots \cite{he_explainer_2022}. Existing approaches are based on surrogate models \cite{he_explainer_2022}, temporal decomposition strategies \cite{xie_explaining_2022, liu_differential_2023}, and model features \cite{fan_gcn-se_2021}.


He et al. \cite{he_explainer_2022} develop a surrogate-based approach. Their explainer adapts PGM-Explainer \cite{vu_pgm-explainer_2020}, a surrogate method that explains static \glspl{gnn}, to explain predictions on \glspl{dtdg}. PGM-Explainer aims to construct an optimal Bayesian network that captures the most relevant graph components. It provides explanations in the form of conditional probabilities. The authors apply PGM-Explainer to each of the graph snapshots in a \gls{dtdg} by treating the snapshots as static graphs. This results in a separate Bayesian network as an explanation for each snapshot. They subsequently aggregate these individual explanations using a pruning strategy to discover explanations that are dominant across the individual explanations.

In contrast, GCN-SE \cite{fan_gcn-se_2021} takes a feature-based approach. GCN-SE itself is not an explainer but rather a graph convolutional network architecture for \glspl{dtdg} with inbuilt explanation capabilities. It employs attention weights for the different graph snapshots. These attention weights are used to assess the importance of the different snapshots to the final prediction. Thus, the GCN-SE approach provides a degree of self-interpretability. However, the attention weights only offer insights into how important certain snapshots are and not into the importance of features within the snapshot, offering only coarse explanations.

Another category of explanation methods utilizes the decomposition approach. DGExplainer \cite{xie_explaining_2022} decomposes the prediction of a \gls{tgnn} by redistributing the prediction score to relevance scores of neurons in the previous neural network layer. This redistribution is repeated backward through the \gls{tgnn} until the input neurons are reached. DGExplainer interprets the redistributed scores as importances for the input features. Liu et al. \cite{liu_differential_2023} also propose a decomposition-based explanation method. They use standard \glspl{gnn} designed for static graphs to make predictions on the different snapshots. Then, they make the predictions understandable across the different snapshots using axiomatic attribution. The authors presume that the predicted distributions exist within a low-dimensional manifold. The approach demonstrates how these predictions change over time as smooth paths on this manifold. The main goal is to enhance interpretability by finding a path on the manifold that accurately represents how the predictions change over time.


\subsection{Explanations on Continuous Time Dynamic Graphs}
\label{s_relatedWork_CTDG}

Explanations tailored for \glspl{ctdg} is an area with very limited prior work. To the best of the author's knowledge, there exist only two explainers that focus on \glspl{ctdg}, named T-GNNExplainer \cite{xia_explaining_2023} and TempME \cite{chen_tempme_2023}. They both use a perturbation approach to provide factual explanations in the form of a subset of past events crucial for the model's predictions.

The underlying perturbation approach of T-GNNExplainer \cite{xia_explaining_2023} is search-based and leverages Monte Carlo Tree Search to find important events. To improve search performance, the authors introduce a navigator component, a multilayer perceptron trained to predict event importances concerning a given prediction. This navigator informs the exploration of the search space. Notably, T-GNNExplainer differs from many perturbation-based explanation methods in static settings in that it does not require users to specify the size of the explanation. Instead, it autonomously identifies an explanation of an appropriate size. T-GNNExplainer can be interpreted as an adaptation of SubgraphX \cite{yuan_explainability_2021}, a search-based perturbation approach for static \glspl{gnn}, to the dynamic setting. In contrast to this predecessor, T-GNNExplainer uses spatial and temporal constraints for its search space and adopts the navigator component for efficient search space explorations.

In contrast, TempME \cite{chen_tempme_2023} is an approach that aims to identify crucial temporal motifs to elucidate the reasoning behind predictions. Temporal motifs are recurrent and statistically significant temporal substructures of the dynamic graph. TempME selects important temporal motifs sampled from a local neighborhood of the nodes involved in the explained prediction. For this selection, it uses a \gls{gnn} pretrained on predicting the importance of motifs to the explained prediction. These importance ratings are used as basis to construct explanations that integrate the collective impact of the identified temporal motifs.

% Usually a \gls{tgnn} $f(\cdot)$ that is tasked to predict the occurrence of future links is fed the examined link $\varepsilon_{t_i}$ alongside all past events $\mathcal{G}(t_i)$. However, in T-GNNExplainer only events in the local k-hop-neighborhood of the prediction target $\varepsilon_{t_i}$ are considered. This is a problem because the omission of the events outside the neighborhood events might result in a different prediction $f(\mathcal{G}(t_i), \varepsilon_{t_i}) \neq f(\mathcal{G}(t_i) \cap N^k_{\varepsilon_{t_i}}, \varepsilon_{t_i})$.


\subsection{Comparison}
\label{s_relateWork_comparison}

While the presented related works provide valuable contributions to explaining predictions of \glspl{tgnn}, they display different shortcomings and leave many possible explanation approaches unexplored. Table \ref{t_relatedWork_Overview} summarizes the key characteristics of the presented approaches. 

\begin{table}[ht]
    \centering
    \footnotesize
    \include{tables/related_work_comparison}
    \caption{Overview of related work compared to the explainer proposed in this thesis. The abbreviations for the tasks are Node Regression (NR), Node Classification (NC), Graph Classification (GC), and Future Link Prediction (FLP). *: The navigator component in T-GNNExplainer requires access to attention weights.}
    \label{t_relatedWork_Overview}
\end{table}

\glspl{tgnn} operating on \glspl{ctdg} are generally more versatile than their \gls{dtdg} counterparts because \glspl{dtdg} can be converted into \glspl{ctdg} without loss of information, but not necessarily the other way around \cite{souza_provably_2022}. Despite this, explanations for \glspl{tgnn} working on \glspl{ctdg} remain understudied \cite{chen_tempme_2023}. T-GNNExplainer \cite{xia_explaining_2023} and TempME \cite{chen_tempme_2023} are the only approaches yet to explore explanations in the context of \glspl{ctdg}.

A major shortcoming of many existing explanation approaches is that they are not model agnostic. A model agnostic explainer applies to any prediction model \cite{prado-romero_survey_2023}; however, most of the related works require a specific class of model, for instance, an attention-based method. This means that such methods cannot be applied to black-box models.

Another factor limiting the applicability of most related explanation approaches is model access. Explanation methods can require access to the prediction model on different levels. Access on the level of gradients, embeddings \cite{verma_counterfactual_2020}, and predictions are differentiated. While approaches that require embedding-wise access only limit the scope of explainable models to those that produce such embeddings, approaches with gradient-wise access restrict the range of explainable models to those that employ accessible neural networks \cite{prado-romero_survey_2023}. A reliance on model access to internal parameters can prohibit the application of an explainer to a novel model, or it might make rigorous adaptations necessary. Novel methods are often designed without any considerations for explainability or specific explainability methods \cite{xia_explaining_2023}, making a non-reliance on model access an asset for an explainer.

Adding to these limitations, none of the existing approaches makes use of counterfactual explanations even though counterfactual explanations have been shown to be highly effective explanations, as they overcome many problems in interpretability and accountability \cite{wachter_counterfactual_2018}. Additionally, counterfactual explanations have already been successfully applied to regular \glspl{gnn} \cite{tan_learning_2022, lucic_cf-gnnexplainer_2022}, making their application in explaining models on dynamic graphs a logical next step.

This thesis advances the capabilities of explanation methods for black-box prediction models on dynamic graphs. It addresses the identified research gap by proposing \acrfull{greedycf} and \\ \acrfull{cftgnn}, two instance-level post-hoc explainers that provide counterfactual explanations for \glspl{tgnn} operating on \glspl{ctdg}. The proposed methods are model agnostic, treating the explained model as a black-box with model access only required on the level of predictions. Both approaches use a search-based perturbation strategy that seeks minimal counterfactual examples. In proposing \gls{greedycf} and \gls{cftgnn}, this thesis provides a valuable addition to the current state of explanation methods for dynamic graphs.

% First build some taxonomical overview of the existing methods, then present them and finally in an overview table compare them and outline how the proposed approach improves on different weaknesses and fills the gap in research. Open to debate is where to localise the problem statement because right now it sits before the related work chapter

