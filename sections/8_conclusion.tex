\section{Conclusion}
\label{s_Conclusion}

Dynamic graphs and \glspl{tgnn} recently garnered attention. Despite the opaque nature of most of the model's operation on dynamic graphs, the explainability of these black-box models remains less explored. Specifically, counterfactual explanations have not been extensively investigated for such models until now. 

Taking a counterfactual perspective into explanations for the predictions of black-box models on dynamic graphs, this thesis proposes \gls{greedycf} and \gls{cftgnn}, two novel explanation methods that provide counterfactual explanations. The explanation problem is framed as a search task where the search space consists of all possible combinations of past events. Removing any of these combinations from the original input may change the prediction. If the removal changes the prediction, it makes that perturbation a counterfactual example.

The search space is constrained in terms of its spatial and temporal extent, and it is structured into a search tree such that it aligns with the objectives of maximizing discoveries of counterfactual examples while minimizing the complexity of the explanation.\gls{greedycf} is proposed as a capable baseline approach for searching for counterfactual explanations in this setting. It leverages a greedy search approach to find counterfactual examples within the search space quickly. As a more sophisticated approach, \gls{cftgnn} is introduced. \gls{cftgnn} adapts a search algorithm based on the Monte Carlo tree search algorithm to efficiently explore different potentially counterfactual perturbations. In the absence of sufficient information on the importance of past events to the explained prediction, both search approaches leverage heuristic selection strategies to guide the search.

An extensive evaluation shows that \gls{cftgnn} provides counterfactual examples more frequently and with lower complexity than \gls{greedycf} or other contemporary explanation approaches while keeping time complexity acceptable. The experiments also uncover that the heuristic selection strategy has a large effect on explanatory performance. The results show that selecting promising events for the input perturbation by recency and ?? is preferable over random selection or selection by spatial distance.

%To find such counterfactual explanations, the explanation problem is framed as a search task. In this framework, the search space is defined as the set of all possible combinations of past events. Each of these combinations represents a perturbation of the original input in which the included events are removed from the input. Changing the input in this way may change the prediction, making the perturbation a counterfactual example. Temporal and spatial constraints are imposed on the search space, and the search space is structured into a search tree such that it aligns with the objectives of maximizing discoveries of counterfactual examples while minimizing the complexity of the explanation. \gls{greedycf} is proposed as a capable baseline approach for searching for counterfactual explanations in this setting. It leverages a greedy search approach to find counterfactual examples within the search space quickly. As a more sophisticated approach, \gls{cftgnn} is introduced. \gls{cftgnn} adapts a search algorithm based on the Monte Carlo tree search algorithm to efficiently explore different potentially counterfactual perturbations. In the absence of sufficient information on the importance of past events to the explained prediction, both search approaches leverage heuristic selection strategies to guide the search. An extensive evaluation shows that \gls{cftgnn} can provide counterfactual examples more frequently and with lower complexity than \gls{greedycf} or other contemporary explanation approaches while keeping time complexity acceptable. It also uncovers that the heuristic selection strategy has a large effect on the explanatory performance. The results show that selecting events for the input pertubation by recency and ?? is preferable over random selection or selection by spatial distance.


\subsection{Limitations}
\label{s_Conclusion_Limitations}

Despite the contribution this work makes, certain conceptual and practical limitations warrant consideration.

Conceptually, the very definition of the explanation problem undertaken in Section \ref{s_ProblemFormulation} limits counterfactual examples to perturbations that remove a set of past events from the input. While this limitation is essential for establishing a clear framework for the search process, it also imposes restrictions. Counterfactual examples do not have to be limited to the omission of information but could also encompass adding events to the input, changing the features associated with the nodes and edges in past events, or changing the timing of events. However, incorporating these dimensions would inevitably result in a substantial expansion of the search space, necessitating the development of novel methods to effectively navigate and address these complex circumstances.

On a practical level, the applicability of \gls{cftgnn} faces two noteworthy constraints. Firstly, the relatively long explanation times and substantial resources required for explaining a single prediction confine the practical use of \gls{cftgnn} to cases where sufficient time is available. \gls{cftgnn} proves impractical for explaining large volumes of predictions within a reasonable timeframe. The main contributing factor to these temporal and resource requirements is the (in-)efficiency of the targeted model. Targeting better-optimized models would thus directly alleviate these concerns. Secondly, the fact that the proposed methods are not guaranteed to find a counterfactual example limits their usefulness. 

% Conceptual
% - Only deletion of past events
% - 

% Evaluation
% - Only one target model
% - Only similar graphs
% - 

% Practical
% - Explicit search approach has long inference times -> unpractical; much energy used
% - Not always an explanation -> Cannot explain every prediction
% - 


\subsection{Future Work}
\label{s_Conclusion_FutureWork}

Future work may address the limitations of this work. Further, it can explore avenues that build upon the presented findings and deepen the understanding of counterfactual explanations in the context of dynamic graphs. 

The evaluation could be expanded by other target models. While \gls{tgn} provides a framework that generalizes many approaches for \glspl{tgnn}, it would be interesting to see if the results gathered in this work generalize to other target models. Assessing the explainers on a more diverse array of datasets could further establish the robustness and applicability of the developed approaches.

Improvements in the performance of the proposed methods could be achieved through systematic hyperparameter tuning, employing techniques such as grid search to optimize the balance between exploration and exploitation in \gls{cftgnn}. 
Additionally, relaxing constraints on explanations, such as allowing the addition of events in perturbations instead of only removal, holds the potential for performance enhancement. However, this would come with the caveats discussed in Section \ref{s_Conclusion_Limitations}.

Lastly, an area that remains to be explored is integrating counterfactual explanation methods with factual ones into a hybrid approach in the dynamic graph context. In doing so, the aspects of necessity and sufficiency would be combined into a single framework.

Lastly, another avenue for future exploration involves integrating counterfactual explanation methods with factual ones in the dynamic graph context, blending aspects of necessity and sufficiency into a unified framework. Such a hybrid approach holds the potential for a more comprehensive understanding of the underlying graph dynamics.

% - Tuning Hyperparameter
% - Combine CF perspective with F perspective


%While the evaluation provides an extensive overview of the capabilities of the proposed explanation methods, it only does so with a single target model. Therefore, it remains unclear whether the findings generalize to other deep models on dynamic graphs.