\section{Related Work}
\label{s_RelatedWork}

% Argue why we only look at the temporal explanation methods, not at any from the static context. Also need to argue why 

As outlined in Chapter \ref{s_ExplainingGNNs}, numerous methods for explaining \glspl{gnn} on static graphs have been developed. However, these models fall short when applied to temporal graph models, as they fail to capture temporal dependencies in dynamic graphs \cite{xia_explaining_2023, he_explainer_2022}. Yet, only a very limited number of past publications have explored explanation methods that take the dynamic nature of \glspl{tgnn} into account. Understanding existing explainability methods in the context of dynamic graphs is crucial for developing and evaluating novel explanation methods for \glspl{tgnn}. Figure \ref{f_taxonomy_explainers} shows an overview of other works that explain \glspl{tgnn}. The methods used for these explainers mainly adapt findings from the static setting to the dynamic context. 

\begin{figure}[ht]
    \centering
    \include{figures/rel_work_overview}
    \label{f_relatedWork_taxonomy}
    \caption{Overview of related work. Some works focus on explaining \glspl{tgnn} operating on the \gls{dtdg} representation, while only one other study explored explanations for \glspl{tgnn} operating on \glspl{ctdg}. The methods are subdivided by their approach, following the taxonomy introduced in Figure \ref{f_taxonomy_explainers}.}
\end{figure}

This chapter provides a structured overview of existing methods and how they compare to the approach developed in this thesis. Distinguishing the approaches at the level of the underlying graph representation, the following sections outline related works for explanations in the \gls{dtdg} (Section \ref{s_relatedWork_DTDG}) and the \gls{ctdg} setting (Section \ref{s_relatedWork_CTDG}), and then derive the research gap from a comparison of the existing approaches in Section \ref{s_relateWork_comparison}.

\subsection{Explanations on Discrete Time Dynamic Graphs}
\label{s_relatedWork_DTDG}

Most prior work focuses on explanations for predictions on \glspl{dtdg}. Explaining such models involves integrating feature importances over the different graph snapshots \cite{he_explainer_2022}. Existing approaches are based on surrogate models \cite{he_explainer_2022}, temporal decomposition strategies \cite{xie_explaining_2022, liu_differential_2023}, and model features \cite{fan_gcn-se_2021}.

He et al. \cite{he_explainer_2022} adapt PGM-Explainer \cite{vu_pgm-explainer_2020} to explain predictions on \glspl{dtdg}. PGM-Explainer is a surrogate method that aims to construct an optimal Bayesian network that captures the most relevant graph components and provides explanations in the form of conditional probabilities. The authors use PGM-Explainer to explain prediction for individual graph snapshot separately, treating each graph snapshot as a static graph. They subsequently aggregate these individual explanations using a pruning strategy to discover dominant, interesting explanations.

In contrast, GCN-SE \cite{fan_gcn-se_2021} takes a different approach. GCN-SE is a graph convolutional network architecture for \glspl{dtdg} that incorporates attention weights for the different graph snapshots. These attention weights are used to assess the importance of the different snapshots to the final prediction. Thus, the GCN-SE approach provides a degree of self-interpretability. However, the attention weights only offer insights into how important certain snapshots are and not into the importance of features within the snapshot, offering only coarse explanations.

Another category of explanation methods utilizes the decomposition approach. DGExplainer \cite{xie_explaining_2022} decomposes the prediction of a \gls{tgnn} by redistributing the prediction score to relevance scores of neurons in the previous neural network layer. This redistribution is repeated backward through the \gls{tgnn} until the input neurons are reached. This provides importance scores for all input features. Liu et al. \cite{liu_differential_2023} also propose a decomposition-based explanation method. They use standard \glspl{gnn} designed for static graphs to make predictions on the different snapshots. Then, they make the predictions smooth and understandable across the different snapshots using axiomatic attribution. The authors presume that the predicted distributions exist within a low-dimensional manifold. The approach visualizes how these predictions change over time as smooth paths on this manifold. The main goal is to enhance interpretability by finding a path that accurately represents how the predictions change over time.


\subsection{Explanations on Continuous Time Dynamic Graphs}
\label{s_relatedWork_CTDG}

Explanations tailored for \glspl{ctdg} is an area with limited prior work. Unlike most related works that focus on \glspl{dtdg}, TGNNExplainer \cite{xia_explaining_2023} uses the \gls{ctdg} representation for dynamic graphs. TGNNExplainer provides factual explanations in the form of a subset of past events crucial for the model's predictions.

The underlying perturbation approach is search-based and leverages Monte Carlo Tree Search to find important events. To improve search performance, the authors introduce a navigator component, a multilayer perceptron trained to predict event importances concerning a given prediction. This navigator informs the exploration of the search space. Notably, TGNNExplainer differs from many perturbation-based explanation methods in static settings in that it does not require users to specify the size of the explanation. Instead, it autonomously identifies an explanation of an appropriate size.

TGNNExplainer can be seen as an adaptation of SubgraphX \cite{yuan_explainability_2021}, a search-based perturbation approach for static \glspl{gnn}, to the dynamic setting. In contrast to this predecessor, TGNNExplainer uses spatial and temporal constraints for its search space and adopts the navigator component for efficient search space explorations.

% Usually a \gls{tgnn} $f(\cdot)$ that is tasked to predict the occurrence of future links is fed the examined link $\varepsilon_{t_i}$ alongside all past events $\mathcal{G}(t_i)$. However, in TGNNExplainer only events in the local k-hop-neighborhood of the prediction target $\varepsilon_{t_i}$ are considered. This is a problem because the omission of the events outside the neighborhood events might result in a different prediction $f(\mathcal{G}(t_i), \varepsilon_{t_i}) \neq f(\mathcal{G}(t_i) \cap N^k_{\varepsilon_{t_i}}, \varepsilon_{t_i})$.


\subsection{Comparison}
\label{s_relateWork_comparison}

While the presented related works provide valuable contributions to explaining predictions of \glspl{tgnn}, they display different shortcomings and leave many possible explanation methods unexplored. Table \ref{t_relatedWork_Overview} summarizes the key characteristics of these approaches. 

\begin{table}[ht]
    \centering
    \footnotesize
    \include{tables/related_work_comparison}
    \caption{Overview of related work compared to the explainer proposed in this thesis. The abbreviations for the tasks are Node Regression (NR), Node Classification (NC), Graph Classification (GC), and Future Link Prediction (FLP). *: The navigator component in TGNNExplainer requires access to attention weights.}
    \label{t_relatedWork_Overview}
\end{table}

\glspl{tgnn} operating on \glspl{ctdg} are generally more versatile than their \gls{dtdg} counterparts because \glspl{dtdg} can be converted into \glspl{ctdg} without loss of information, but not necessarily the other way around \cite{souza_provably_2022}. Nonetheless, explanations for \glspl{tgnn} working on \glspl{ctdg} remain heavily understudied. TGNNExplainer \cite{xia_explaining_2023} is the only approach yet to explore explanations in the context of this representation.

A major shortcoming of many existing explanation approaches is that they are not model agnostic. A model agnostic explainer applies to any prediction model \cite{prado-romero_survey_2023}; however, most of the related works require a specific class of model, for instance, an attention-based method.

Another factor limiting the applicability of most related explanation approaches is model access. Explanation methods can require access to the prediction model on different levels. Access on the level of gradients, embeddings \cite{verma_counterfactual_2020}, and predictions are differentiated. While approaches that require embedding-wise access only limit the scope of explainable models to those that produce such embeddings, approaches with gradient-wise access restrict the range of explainable models to those that employ neural networks \cite{prado-romero_survey_2023}. A reliance on model access to internal parameters can prohibit the application of an explainer to a novel model, or it might make rigorous adaptations necessary. Novel methods are often designed without any considerations for explainability or specific explainability methods \cite{xia_explaining_2023}, making a non-reliance on model access an asset for an explainer.

Furthermore, none of the existing approaches makes use of counterfactual explanations even though counterfactual explanations have been shown to be highly effective explanations, as they overcome many problems in interpretability and accountability \cite{wachter_counterfactual_2018}. Additionally, counterfactual explanations have already been successfully applied to regular \glspl{gnn} \cite{tan_learning_2022, lucic_cf-gnnexplainer_2022}, but their potential in dynamic graph contexts remains unexplored.

This thesis improves on existing explanation methods for prediction models on dynamic graphs. It addresses the identified research gap by proposing \gls{cftgnn}, an instance-level post-hoc explainer that provides counterfactual examples as explanations for \glspl{tgnn} operating on \glspl{ctdg}. \gls{cftgnn} is model agnostic, treating the explained model as a black-box with model access only required on the level of predictions. It uses a search-based perturbation strategy that seeks to find minimal counterfactual examples. In proposing \gls{cftgnn}, this thesis advances the current state of explanation methods for dynamic graphs.

% First build some taxonomical overview of the existing methods, then present them and finally in an overview table compare them and outline how the proposed approach improves on different weaknesses and fills the gap in research. Open to debate is where to localise the problem statement because right now it sits before the related work chapter

